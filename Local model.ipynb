{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17zZMOQ-SeRLHrIoLBGC6eLJ1gQK2OOPj","timestamp":1728081708637},{"file_id":"1WJJ5fOtVy0-nhEEdnMGxBYcFgRUB3xdd","timestamp":1628050872356},{"file_id":"1ZrIbbqjiugMWDPBOMTs_efiIM9HF0aWS","timestamp":1627072415432}],"mount_file_id":"1WJJ5fOtVy0-nhEEdnMGxBYcFgRUB3xdd","authorship_tag":"ABX9TyNhYLaCqDitUcHMsJ5XVzUX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"v_e2ougGgLAp"},"source":["**A Semantic-Proximity Term-Weighting Scheme for Aspect Category Detection**\n","\n","Vázquez-Hernández, M., Villaseñor-Pineda, L., & Montes, M. (2022). A Semantic-Proximity Term-Weighting Scheme for Aspect Category Detection. Procesamiento del Lenguaje Natural, 69, 117-127."]},{"cell_type":"markdown","metadata":{"id":"El-2WsOxgUEZ"},"source":["**Librerías, Clases y Métodos**\n","---"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGalPhxqtG_X","executionInfo":{"status":"ok","timestamp":1728081880332,"user_tz":360,"elapsed":21270,"user":{"displayName":"Monserrat Vázquez Hernández","userId":"12458819110641506711"}},"outputId":"9f65d622-ed12-4da1-89db-80742255800e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Github/ABAA-model/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4TOojamtMMU","executionInfo":{"status":"ok","timestamp":1728081882200,"user_tz":360,"elapsed":258,"user":{"displayName":"Monserrat Vázquez Hernández","userId":"12458819110641506711"}},"outputId":"39b7a7d7-3a09-4781-cc3c-a45601696698"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/ABAA-model\n"]}]},{"cell_type":"code","metadata":{"id":"xaXU8hMogHWS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728081934548,"user_tz":360,"elapsed":6997,"user":{"displayName":"Monserrat Vázquez Hernández","userId":"12458819110641506711"}},"outputId":"4deb9dd7-0854-482d-aa1f-d597028fbe3d"},"source":["# SS3 Model https://github.com/sergioburdisso/pyss3\n","pip install pySS3"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pySS3\n","  Downloading pyss3-0.6.4-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pySS3) (1.16.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from pySS3) (3.0.11)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from scikit-learn[alldeps]>=0.20->pySS3) (1.5.2)\n","Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.10/dist-packages (from pySS3) (4.66.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pySS3) (3.7.1)\n","Collecting iterative-stratification (from pySS3)\n","  Downloading iterative_stratification-0.1.7-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-learn[alldeps]>=0.20->pySS3) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-learn[alldeps]>=0.20->pySS3) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-learn[alldeps]>=0.20->pySS3) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-learn[alldeps]>=0.20->pySS3) (3.5.0)\n","\u001b[33mWARNING: scikit-learn 1.5.2 does not provide the extra 'alldeps'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pySS3) (2.8.2)\n","Downloading pyss3-0.6.4-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n","Installing collected packages: iterative-stratification, pySS3\n","Successfully installed iterative-stratification-0.1.7 pySS3-0.6.4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-5cqOX-gY9_","executionInfo":{"status":"ok","timestamp":1728081943595,"user_tz":360,"elapsed":5933,"user":{"displayName":"Monserrat Vázquez Hernández","userId":"12458819110641506711"}},"outputId":"291e2ce6-0201-4da1-c8ce-25c28c39db22"},"source":["from pyss3 import SS3\n","from pyss3.util import Dataset, Evaluation, span\n","from pyss3.server import Live_Test\n","\n","from sklearn.metrics import accuracy_score\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","#-----------------------\n","import numpy as np\n","import math\n","\n","#-----------------------\n","#undersampling\n","import re\n","import pandas\n","from collections import defaultdict\n","from pandas import DataFrame\n","\n","#-----------------------\n","from gensim.models import Word2Vec\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","from gensim.models import KeyedVectors\n","\n","#Classes used\n","class Category:\n","  def __init__(self,category):\n","        self.category = category\n","        self.entity = \"\"\n","        self.aspect = \"\"\n","        self.reviews = [] #Todos los reviews de la categoria\n","        self.positivos= Polaridad(\"positivos\") #Información de la clase positiva (reviews, vocabulario, vector)\n","        self.negativos = Polaridad(\"negativos\") #Información de la clase positiva (reviews, vocabulario, vector)\n","        self.neutros = Polaridad(\"neutros\") #Información de la clase positiva (reviews, vocabulario, vector)\n","\n","class Polaridad:\n","  def __init__(self,Polaridad):\n","    self.polaridad = Polaridad\n","    self.reviews = []\n","    self.vocabulario = []\n","    self.vector = []\n","    self.promedio = 0\n","    self.dev_std = 0\n","    self.umbral = 0\n","    self.diccionario_gv = dict() #Diccionario para valores globales\n","\n","class Review:\n","  def __init__(self,review):\n","    self.review = review\n","    self.review_clean = ''\n","    self.polaridad = ''\n","    self.vector_cat = []\n","\n","class PCategory:\n","  def __init__(self, category):\n","    self.category = category\n","    self.Reviews = []\n","\n","class PReview:\n","  def __init__(self, review, no_review_, polaridad_):\n","    self.no_review_ = no_review_\n","    self.review_ = review\n","    self.polaridad_ = polaridad_\n","    self.prob_neg = 0\n","    self.prob_pos = 0\n","    self.prob_neu = 0\n","    self.res_clasificacion = \"\"\n","\n","def CleanReview(review):\n","    ST = stopwords.words('english') #stop words\n","    ST.append(\"iban\")\n","    ST.append(\"has\")\n","    ST.append(\"ahi\")\n","    ST.append(\"ven\")\n","    ST.append(\"si\")\n","    ST.append(\"otro\")\n","    ST.append(\"esa\")\n","    ST.append(\"etc\")\n","    ST.append(\"mas\")\n","    ST.append(\"asi\")\n","    ST.append(\"uno\")\n","    ST.append(\"unas\")\n","    ST.append(\"un\")\n","    ST.append(\"vez\")\n","    ST.append(\"ser\")\n","    ST.append(\"dia\")\n","    ST.append(\"aun\")\n","    ST.append(\"pues\")\n","    ST.append(\"de\")\n","    ST.append(\"hola\")\n","    ST.append(\"haber\")\n","\n","    AuxReview = review.lower().replace(\"\\n\",\"\").replace(',',' ').replace('!',' ').replace(':',' ').replace('(',' ').replace(')',' ').replace('.',' ').replace('$t$s',' ').replace('$t$','').replace('\"',' ').replace('?',' ').replace('¿',' ').replace('!',' ').replace('¡',' ').replace('(…)',' ').replace('“”',' ').replace('″',' ').replace(';',' ').replace('-',' ').replace('//',' ')\n","    AuxReview = AuxReview.replace('<',' ').replace('+',' ').replace('>',' ').replace('”',' ').replace('“',' ').replace('”',' ').replace('//',' ').replace('…',' ').replace('’',' ').replace('º',' ').replace('‘',' ')\n","    AuxReview = AuxReview.replace('[',' ').replace(']',' ').replace('%',' ').replace(\"/\",\" \").replace(\"d´\",\"d\")\n","    AuxReview = AuxReview.replace('  ',' ').replace('€','').replace('0','').replace('1','').replace('2','').replace('3','').replace(\"l'o\",\"o\").replace('4','').replace('5','').replace('6','').replace('7','').replace('8','').replace('9','')\n","    AuxReview = AuxReview.replace('á','a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u').replace(\"*\",\" \").replace(\"-\",\" \")\n","\n","    #Get tokens from review\n","    tokens = AuxReview.split(' ')\n","\n","    #Remove stopwords\n","    clean_tokens = tokens[:]\n","    for token in tokens:\n","        if token in ST:\n","            clean_tokens.remove(token)\n","\n","    clean_review = \"\"\n","    for token in clean_tokens:\n","        clean_review += token + ' '\n","\n","    return clean_review.replace(\"  \",\" \")\n","\n","def CleanDocs(documentos):\n","    clean_Docs = []\n","    for doc in documentos:\n","        clean_Docs.append(CleanReview(doc))\n","\n","    return clean_Docs\n","\n","def cosine_distance(u, v):\n","    \"\"\" Returns the cosine of the angle between vectors v and u. This is equal to uv / |u||v|. \"\"\"\n","    return np.dot(u, v) / (math.sqrt(np.dot(u,u) * math.sqrt(np.dot(v,v))))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"3oM-fe66gf3R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627072470708,"user_tz":300,"elapsed":197,"user":{"displayName":"Monserrat Vázquez Hernández","photoUrl":"","userId":"12458819110641506711"}},"outputId":"2d133d3e-84ce-4ba3-d65d-837991245581"},"source":["#-----------------------\n","#SVM\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.svm import SVC\n","from sklearn import metrics\n","from sklearn.decomposition import PCA\n","from sklearn.feature_extraction import DictVectorizer, FeatureHasher\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","\n","#random undersampling\n","import numpy as np\n","import re\n","import pandas\n","from collections import defaultdict\n","from pandas import DataFrame\n","\n","#random oversampling\n","from imblearn.over_sampling import RandomOverSampler\n","#---------------------------------\n","import re\n","from scipy import spatial"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"s-QjWULUghQF"},"source":["#-------------------------\n","#CNN-LSTM\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Bidirectional\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import Activation\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from keras.models import Model\n","from keras.layers import *\n","from keras.initializers import Constant\n","\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aE8WMNm-gipH"},"source":["#SIMILITUD COSENO\n","import re\n","from scipy import spatial\n","\n","def cos_sim(u,v):\n","  result = 1 - spatial.distance.cosine(u, v)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QeuPAXx8gj8K"},"source":["\"\"\"SVM\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n","#Count words frequency in documents\n","def tokens(doc):\n","  return(tok.lower() for tok in re.findall(r\"\\w+\", doc))\n","\n","def frequency(tokens, review,vector_categoria, vocabulario):\n","  f = dict.fromkeys(vocabulario,0)\n","  # print(vector_categoria)\n","  # print(f)\n","  for token in tokens:\n","    if token in model:\n","      if token in f:\n","        f[token] += cos_sim(vector_categoria,model[token]) #1\n","\n","  vector = []\n","  for item in vocabulario:\n","    vector.append(f[item])\n","\n","  # print(vector)\n","  return vector\n","\n","def tokens_frequency(doc,vector_categoria,Vocabulario):\n","  # print(vector_categoria)\n","  return frequency(tokens(doc),tokens(doc),vector_categoria,Vocabulario)\n","\n","\n","#Todo el vocabulario observado en los reviews de entrenamiento\n","def Vocabulario_train(Reviews_train):\n","  Vocabulario_train = []\n","  for review in Reviews_train:\n","    tokens = review.review.split(' ')\n","    for token in tokens:\n","      if token not in Vocabulario_train:\n","        Vocabulario_train.append(token)\n","\n","  return Vocabulario_train\n","\n","\n","#Todo el vocabulario observado en los reviews de entrenamiento\n","def Vocabulario_train_Over(Reviews_train):\n","  Vocabulario_train = []\n","  for review in Reviews_train:\n","    tokens = review.split(' ')\n","    for token in tokens:\n","      if token not in Vocabulario_train:\n","        Vocabulario_train.append(token)\n","\n","  return Vocabulario_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JgLoVuG8gnZs"},"source":["GloVe: Embeddings\n","---"]},{"cell_type":"markdown","metadata":{"id":"upl-ax4vgn9S"},"source":["GloVe: Embeddings Wikipedia\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"b-6CaM2-gu8w"},"source":["GloVe: Embeddings Twitter\n","\n","\n","---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQORzDdMgvUF","executionInfo":{"status":"ok","timestamp":1627074872470,"user_tz":300,"elapsed":272657,"user":{"displayName":"Monserrat Vázquez Hernández","photoUrl":"","userId":"12458819110641506711"}},"outputId":"e1bcbd52-f43a-4eee-9150-45dfece9864f"},"source":["#-------------------------------------\n","\"\"\"Twitter\"\"\"\n","word2vec_output_file = './Files/glove.twitter.27B.200d.txt.word2vec'\n","# word2vec_output_file = '/content/drive/MyDrive/glove.twitter.27B.25d.txt.word2vec'\n","# glove2word2vec(glove_input_file, word2vec_output_file)\n","model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n","\n","embedding_size = 200"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:494: DeprecationWarning: This function is deprecated.  See https://github.com/RaRe-Technologies/smart_open/blob/develop/MIGRATING_FROM_OLDER_VERSIONS.rst for more information\n","  warnings.warn(message, category=DeprecationWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"92Hu0qvsgzoE"},"source":["Métodos para la extracción de léxico por categoría-polaridad\n","---"]},{"cell_type":"markdown","metadata":{"id":"BCrpzhEYg0Np"},"source":["**Extracción** de léxico por categoría por polaridad\n","\n","---"]},{"cell_type":"code","metadata":{"id":"o3fxrT80g3X9"},"source":["import statistics\n","\n","# category = 'LOCATION#GENERAL'\n","# if True:\n","def Lexicos_polaridad_categoria(category):\n","  print(category)\n","  clf = SS3()\n","\n","  #------------------------------------------------------------------------------------------------------------\n","  #Cargar datos de entrenamiento\n","  d_train = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TRAIN.reviews.gold','r', encoding='utf-8')\n","  d_train_ = []\n","  for line in d_train.readlines():\n","    d_train_.append(line.replace(\"\\n\",\"\"))\n","  d_train.close()\n","\n","  #Cargar clases de entrenamiento\n","  l_train = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TRAIN.class.gold','r', encoding='utf-8')\n","\n","  l_train_ = []\n","  for line in l_train.readlines():\n","    l_train_.append(line.replace(\"\\n\",\"\"))\n","\n","  l_train.close()\n","\n","  #Cargar polaridades de entrenamiento\n","  p_train = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TRAIN.polarities.gold','r', encoding='utf-8')\n","\n","  p_train_ = []\n","  for line in p_train.readlines():\n","    p_train_.append(line.replace(\"\\n\",\"\"))\n","\n","  p_train.close()\n","\n","  # print(p_train_)\n","\n","  x_train = []\n","  y_train = []\n","\n","  for index_review in range(len(d_train_)):\n","    if l_train_[index_review] == category and p_train_[index_review] != \"neutral\" and p_train_[index_review] != \"conflict\":\n","    # if l_train_[index_review] == category and p_train_[index_review] != \"conflict\":\n","      x_train.append(d_train_[index_review])\n","      y_train.append(p_train_[index_review])\n","\n","  x_train= CleanDocs(x_train)\n","\n","  #------------------------------------------------------------------------------------------------------------\n","\n","\n","  #------------------------------------------------------------------------------------------------------------\n","  #Cargar datos de entrenamiento\n","  d_test = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TEST.reviews.gold','r', encoding='utf-8')\n","  d_test_ = []\n","  for line in d_test.readlines():\n","    d_test_.append(line.replace(\"\\n\",\"\"))\n","  d_test.close()\n","\n","  #Cargar clases de entrenamiento\n","  l_test = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TEST.class.gold','r', encoding='utf-8')\n","\n","  l_test_ = []\n","  for line in l_test.readlines():\n","    l_test_.append(line.replace(\"\\n\",\"\"))\n","  l_test.close()\n","\n","  #Cargar polaridades de entrenamiento\n","  p_test = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TEST.polarities.gold','r', encoding='utf-8')\n","\n","  p_test_ = []\n","  for line in p_test.readlines():\n","    p_test_.append(line.replace(\"\\n\",\"\"))\n","\n","  p_test.close()\n","\n","\n","  x_test = []\n","  y_test = []\n","\n","  for index_review in range(len(d_test_)):\n","    if l_test_[index_review] == category and p_test_[index_review] != \"neutral\" and p_test_[index_review] != \"conflict\":\n","    # if l_test_[index_review] == category and p_test_[index_review] != \"conflict\":\n","      x_test.append(d_test_[index_review])\n","      y_test.append(p_test_[index_review])\n","\n","  x_test= CleanDocs(x_test)\n","  #------------------------------------------------------------------------------------------------------------\n","\n","\n","  # #Cargar datos de entrenamiento\n","  # directorio = \"/content/drive/MyDrive/ProyectoTesis_INAOE/Slot3/restaurant/train/\" + category\n","  # x_train, y_train = Dataset.load_from_files(directorio, folder_label=False)\n","  # x_train= CleanDocs(x_train)\n","\n","  # #Cargar datos de pruebas\n","  # directorio = \"/content/drive/MyDrive/ProyectoTesis_INAOE/Slot3/restaurant/test/\" + category\n","  # x_test, y_test = Dataset.load_from_files(directorio, folder_label=False)\n","  # x_test = CleanDocs(x_test)\n","\n","  clf.train(x_train, y_train) #Entrenar para calcular pesos a la forma SS3\n","  # Evaluation.test(clf, x_test, y_test)\n","\n","  s_vals=[0.2 , 0.32, 0.44, 0.56, 0.68, 0.8]\n","  l_vals=[0.1 , 0.48, 0.86, 1.24, 1.62, 2]\n","  p_vals=[1.75, 1.95, 2.15, 2.35, 2.55, 2.75]\n","  best_s, best_l, best_p, _ = Evaluation.grid_search(clf,x_test, y_test, s=s_vals,l=l_vals,p=p_vals)\n","\n","  # print(\"The hyperparameter values that obtained the best Accuracy are:\")\n","  # print(\"Smoothness(s):\", best_s)\n","  # print(\"Significance(l):\", best_s)\n","  # print(\"Sanction(p):\", best_p)\n","\n","  clf.set_hyperparameters(best_s, best_s, best_p, 0.0)\n","  clf.train(x_train, y_train) #Entrenar para calcular pesos a la forma SS3\n","\n","  polaridades = clf.get_categories() #CategoriasIdentificadas\n","  ctr = 0\n","\n","\n","\n","  #Auxiliares para definir el umbral a considerar\n","  ValoresGlobales = []\n","  Diccionario_terminos = dict()\n","  suma = 0\n","  promedio = 0\n","  dev_std = 0\n","\n","\n","  Vocabulario_polaridades_train = []\n","\n","  for polaridad in polaridades:\n","    # print(polaridad)\n","    if polaridad != 'conflict' and polaridad != \"neutral\":\n","      CPolaridad = Polaridad(polaridad)\n","      vocab_ss3 = clf.__get_category_vocab__(ctr)\n","      freqFile= open(polaridad + '.txt','w', encoding='utf-8')\n","      vocabulario = []\n","      for item in vocab_ss3:\n","        # if len(item[0]) >=2:\n","        vocabulario.append(item[0])\n","        freqFile.write(item[0] + ' ' + str(item[4]) + '\\n')\n","        ValoresGlobales.append(item[4])\n","        Diccionario_terminos[item[0]] = item[4]\n","\n","      freqFile.close()\n","\n","      #Auxiliares para variables globales\n","      suma = sum(ValoresGlobales)\n","      promedio = suma/len(ValoresGlobales)\n","      if len(ValoresGlobales) > 1 :\n","        dev_std = statistics.stdev(ValoresGlobales)\n","      else:\n","        dev_std = ValoresGlobales[0]\n","\n","\n","      CPolaridad.diccionario_gv = Diccionario_terminos\n","      CPolaridad.promedio = promedio\n","      CPolaridad.dev_std = dev_std\n","      CPolaridad.vocabulario = vocabulario\n","      Vocabulario_polaridades_train.append(CPolaridad)\n","    ctr += 1\n","\n","  return Vocabulario_polaridades_train, x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lo_uQGZkhtp5"},"source":["Vector Único\n","\n","---"]},{"cell_type":"code","metadata":{"id":"jnV_NtXDhu67"},"source":["def VectorUnico(vocabulario, gv, x, promedio, dev_std, e_umbral):\n","  #-----------------------\n","  #Librerías\n","  import numpy as np\n","  import math\n","\n","  if e_umbral == True:\n","    umbral = promedio + (x*dev_std)\n","    print(\"Umbral: \" + str(umbral))\n","  else:\n","    umbral = 0\n","\n","\n","  contador = 0\n","  index = 0\n","\n","\n","  #construcción vector único\n","  for item in vocabulario:\n","    if gv[item] >= umbral and item in model:\n","      contador += 1\n","\n","      if index == 0:\n","        index += 1\n","        Vector = model[item]\n","      else:\n","        Vector = np.asarray(Vector + model[item])\n","\n","  #Vector único promedio\n","  CVector = np.asarray([n * (1/contador) for n in Vector])\n","  # print(\"tÉRMINOS: \" + str(contador))\n","  return CVector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYzFjqwgiTgI"},"source":["# SVM"]},{"cell_type":"code","metadata":{"id":"Bl9poAHCiVvj"},"source":["import re\n","from collections import defaultdict\n","\n","def SVM(categoria, x_train, y_train, x_test, y_test):\n","  print(categoria.category)\n","\n","  polaridades = [\"negative\",\"positive\"] #No se debe considerar la polaridad NEUTRAL\n","\n","  #Agrupar información de reviews en un solo conjunto de datos\n","  Reviews_train_ = []\n","  Label_train_ = []\n","  contador = 0\n","  flag = 0\n","  positivos = 0\n","  negativos = 0\n","\n","\n","  vec_negative = categoria.negativos.vector\n","  vec_positive = categoria.positivos.vector\n","\n","  # print(y_train)\n","\n","  for doc in x_train:\n","    if y_train[contador] != \"neutral\" and y_train[contador] != \"conflict\":\n","      review = Review(doc)\n","      review.polaridad = y_train[contador].lower()\n","\n","      if y_train[contador] == \"negative\":\n","        negativos += 1\n","        review.vector_cat = vec_negative\n","      else:\n","        if y_train[contador] == \"positive\":\n","          positivos += 1\n","          review.vector_cat = vec_positive\n","\n","\n","      Label_train_.append(y_train[contador])\n","      Reviews_train_.append(review)\n","\n","    contador += 1\n","\n","\n","\n","\n","  if len(Reviews_train_) > 0:\n","    Vocabulario = Vocabulario_train(Reviews_train_)\n","\n","\n","    #Datos de test\n","    Reviews_test_ = []\n","    Label_test_ = []\n","    contador = 0\n","\n","    for doc in x_test:\n","      if y_test[contador] != \"neutral\" and y_test[contador] != \"conflict\":\n","        review = Review(doc)\n","        review.polaridad = y_test[contador].lower()\n","\n","        if y_test[contador] == \"negative\":\n","          review.vector_cat = categoria.negativos.vector\n","        else:\n","          review.vector_cat = categoria.positivos.vector\n","\n","\n","        Label_test_.append(y_test[contador])\n","        Reviews_test_.append(review)\n","\n","\n","      contador += 1\n","\n","\n","\n","    print(\"INICIA SVM Y PREPARACIÓN DE DATOS\")\n","\n","    X_train = [tokens_frequency(d.review,d.vector_cat,Vocabulario) for d in Reviews_train_]\n","    # svc = SVC(C=2.5, kernel='linear', degree=3, gamma='auto').fit(X_train,Label_train_)\n","\n","    svc = SVC(kernel='linear', C=2.0, decision_function_shape='ovo',probability=True).fit(X_train,Label_train_)\n","\n","    # svc = SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train,Label_train_)\n","\n","\n","\n","    X_test = [tokens_frequency(d.review,d.vector_cat,Vocabulario) for d in Reviews_test_]\n","\n","    pred = svc.predict(X_test)\n","    # print(svc.predict_proba(X_test))\n","    # print(pred)\n","    y__test = Label_test_\n","    print(confusion_matrix(Label_test_, pred))\n","    print('accuracy score %0.3f' % svc.score(X_test, y__test))\n","    print(metrics.classification_report(y__test,pred) )\n","\n","    # summary(svm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"emY08QLriYkx"},"source":["import re\n","from collections import defaultdict\n","\n","def SVM_binario(categoria, x_train, y_train, x_test, y_test):\n","  print(categoria.category)\n","\n","  ##############################################################\n","  #Datos de clasificación\n","  PCategory_ = PCategory(categoria.category)\n","\n","  PReviews = []\n","  contador = 0\n","  for review in x_test:\n","    PReview_ = PReview(review,contador,y_test[contador])\n","    PReviews.append(PReview_)\n","    contador += 1\n","\n","\n","\n","\n","\n","  polaridades = [\"negative\",\"positive\"] #No se debe considerar la polaridad NEUTRAL\n","\n","  for polaridad in polaridades:\n","    print(polaridad)\n","    #Agrupar información de reviews en un solo conjunto de datos\n","    Reviews_train_ = []\n","    Label_train_ = []\n","    contador = 0\n","    flag = 0\n","    positivos = 0\n","    negativos = 0\n","\n","\n","    if polaridad == 'negative':\n","      vec_categoria = categoria.negativos.vector\n","    else:\n","      vec_categoria = categoria.positivos.vector\n","\n","    # print(y_train)\n","\n","    for doc in x_train:\n","      if y_train[contador] != \"neutral\" and y_train[contador] != \"conflict\":\n","        if y_train[contador] == polaridad:\n","          Label_train_.append(1)\n","        else:\n","          Label_train_.append(0)\n","\n","        Reviews_train_.append(doc)\n","\n","      contador += 1\n","\n","\n","    if len(Reviews_train_) > 0:\n","      Vocabulario = Vocabulario_train_Over(Reviews_train_)\n","\n","\n","      #Datos de test\n","      Reviews_test_ = []\n","      Label_test_ = []\n","      contador = 0\n","\n","      for doc in x_test:\n","        if y_test[contador] != \"neutral\" and y_test[contador] != \"conflict\":\n","          review = Review(doc)\n","          review.polaridad = y_test[contador].lower()\n","          review.vector_cat = vec_categoria\n","\n","          if y_test[contador] == polaridad:\n","            Label_test_.append(1)\n","          else:\n","            Label_test_.append(0)\n","\n","          Reviews_test_.append(review)\n","        contador += 1\n","\n","\n","\n","      print(\"INICIA SVM Y PREPARACIÓN DE DATOS\")\n","\n","      X_train = [tokens_frequency(d,vec_categoria,Vocabulario) for d in Reviews_train_]\n","      svc = SVC(C=2.5, kernel='linear', degree=2, gamma='auto',probability=True).fit(X_train,Label_train_)\n","\n","      # svc = SVC(kernel='linear', C=2.5, decision_function_shape='ovo').fit(X_train,Label_train_)\n","      # svc = SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train,Label_train_)\n","\n","\n","\n","      X_test = [tokens_frequency(d.review,vec_categoria,Vocabulario) for d in Reviews_test_]\n","\n","      pred = svc.predict(X_test)\n","\n","      # Datos de predicción\n","      probabilidades = svc.predict_proba(X_test)\n","      for i in range(len(x_test)):\n","        if polaridad == 'negative':\n","          PReviews[i].prob_neg = probabilidades[i][1]\n","        else:\n","          PReviews[i].prob_pos = probabilidades[i][1]\n","\n","\n","      y__test = Label_test_\n","      # print(confusion_matrix(Label_test_, pred))\n","      # print('accuracy score %0.3f' % svc.score(X_test, y__test))\n","      # print(metrics.classification_report(y__test,pred) )\n","\n","  PCategory_.Reviews = PReviews\n","  return PCategory_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CpZ5yaD-iaDU"},"source":["import re\n","from collections import defaultdict\n","from collections import Counter\n","\n","def SVM_OverSampling(categoria, x_train, y_train, x_test, y_test):\n","  print(categoria.category)\n","\n","  polaridades = [\"negative\",\"positive\"] #No se debe considerar la polaridad NEUTRAL\n","\n","  #Agrupar información de reviews en un solo conjunto de datos\n","  Reviews_train_ = []\n","  Label_train_ = []\n","  contador = 0\n","  flag = 0\n","  positivos = 0\n","  negativos = 0\n","\n","\n","  vec_negative = categoria.negativos.vector\n","  vec_positive = categoria.positivos.vector\n","\n","  # print(y_train)\n","\n","  for doc in x_train:\n","    if y_train[contador] != \"neutral\" and y_train[contador] != \"conflict\":\n","      Label_train_.append(y_train[contador])\n","      Reviews_train_.append(doc)\n","\n","    contador += 1\n","\n","\n","  if len(Reviews_train_) > 0:\n","    Vocabulario = Vocabulario_train_Over(Reviews_train_)\n","\n","    ###############################################################\n","    # Oversampling strategy - 21/03/21\n","    # instantiating the random over sampler\n","    ros = RandomOverSampler()\n","\n","    # resampling X, y\n","    X = DataFrame(Reviews_train_,columns=['review'])\n","\n","    X_ros, y_ros = ros.fit_resample(X, Label_train_)\n","\n","    # new class distribution\n","    print(Counter(y_ros))\n","    # print(y_ros)\n","\n","    Reviews_train_Over = []\n","    Label_train_over = []\n","    for i in range(len(X_ros)):\n","      review = Review(X_ros[i][0])\n","\n","      if y_ros[i] == \"negative\":\n","        review.vector_cat = vec_negative\n","      else:\n","        if y_ros[i] == \"positive\":\n","          review.vector_cat = vec_positive\n","\n","      Label_train_over.append(y_ros[i])\n","      Reviews_train_Over.append(review)\n","\n","\n","    ################################################################\n","\n","\n","\n","    #Datos de test\n","    Reviews_test_ = []\n","    Label_test_ = []\n","    contador = 0\n","\n","    for doc in x_test:\n","      if y_test[contador] != \"neutral\" and y_test[contador] != \"conflict\":\n","        review = Review(doc)\n","        review.polaridad = y_test[contador].lower()\n","\n","        if y_test[contador] == \"negative\":\n","          review.vector_cat = categoria.negativos.vector\n","        else:\n","          review.vector_cat = categoria.positivos.vector\n","\n","\n","        Label_test_.append(y_test[contador])\n","        Reviews_test_.append(review)\n","\n","\n","      contador += 1\n","\n","\n","\n","    print(\"INICIA SVM Y PREPARACIÓN DE DATOS\")\n","\n","    X_train = [tokens_frequency(d.review,d.vector_cat,Vocabulario) for d in Reviews_train_Over]\n","    # svc = SVC(C=2.5, kernel='linear', degree=3, gamma='auto',probability=True).fit(X_train,Label_train_over)\n","\n","    svc = SVC(kernel='linear', C=2, decision_function_shape='ovo',probability=True).fit(X_train,Label_train_over)\n","    # svc = SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train,Label_train_)\n","\n","    X_test = [tokens_frequency(d.review,d.vector_cat,Vocabulario) for d in Reviews_test_]\n","\n","    pred = svc.predict(X_test)\n","    y__test = Label_test_\n","    print(confusion_matrix(Label_test_, pred))\n","    print('accuracy score %0.3f' % svc.score(X_test, y__test))\n","    print(metrics.classification_report(y__test,pred) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3aCb7mUibiP"},"source":["import re\n","from collections import defaultdict\n","from collections import Counter\n","\n","def SVM_OverSampling_binario(categoria, x_train, y_train, x_test, y_test):\n","  print(categoria.category)\n","\n","  ##############################################################\n","  #Datos de clasificación\n","  PCategory_ = PCategory(categoria.category)\n","\n","  PReviews = []\n","  contador = 0\n","  for review in x_test:\n","    PReview_ = PReview(review,contador,y_test[contador])\n","    PReviews.append(PReview_)\n","    contador += 1\n","\n","\n","\n","\n","  polaridades = [\"negative\",\"positive\"] #No se debe considerar la polaridad NEUTRAL\n","\n","  for polaridad in polaridades:\n","    print(polaridad)\n","    # Agrupar información de reviews en un solo conjunto de datos\n","    Reviews_train_ = []\n","    Label_train_ = []\n","    contador = 0\n","    flag = 0\n","    positivos = 0\n","    negativos = 0\n","\n","\n","    if polaridad == 'negative':\n","      vec_categoria = categoria.negativos.vector\n","    else:\n","      vec_categoria = categoria.positivos.vector\n","\n","  # print(y_train)\n","\n","    for doc in x_train:\n","      if y_train[contador] != \"neutral\" and y_train[contador] != \"conflict\":\n","      # if y_train[contador] != \"conflict\":\n","        if y_train[contador] == polaridad:\n","          Label_train_.append(1)\n","        else:\n","          Label_train_.append(0)\n","\n","        Reviews_train_.append(doc)\n","\n","      contador += 1\n","\n","\n","    if len(Reviews_train_) > 0:\n","      Vocabulario = Vocabulario_train_Over(Reviews_train_)\n","\n","      ###############################################################\n","      # Oversampling strategy - 21/03/21\n","      # instantiating the random over sampler\n","      ros = RandomOverSampler()\n","\n","      # resampling X, y\n","      X = DataFrame(Reviews_train_,columns=['review'])\n","\n","      X_ros, y_ros = ros.fit_resample(X, Label_train_)\n","\n","      # new class distribution\n","      # print(Counter(y_ros))\n","      # print(y_ros)\n","\n","      Reviews_train_Over = []\n","      Label_train_over = []\n","      for i in range(len(X_ros)):\n","        review = Review(X_ros[i][0])\n","        review.vector_cat = vec_categoria\n","\n","        Label_train_over.append(y_ros[i])\n","        Reviews_train_Over.append(review)\n","\n","\n","      ################################################################\n","\n","\n","\n","      #Datos de test\n","      Reviews_test_ = []\n","      Label_test_ = []\n","      contador = 0\n","\n","      for doc in x_test:\n","        if y_test[contador] != \"neutral\" and y_test[contador] != \"conflict\":\n","        # if y_test[contador] != \"conflict\":\n","          review = Review(doc)\n","          review.polaridad = y_test[contador].lower()\n","          review.vector_cat = vec_categoria\n","\n","          if y_test[contador] == polaridad:\n","            Label_test_.append(1)\n","          else:\n","            Label_test_.append(0)\n","\n","          Reviews_test_.append(review)\n","        contador += 1\n","\n","\n","\n","      print(\"INICIA SVM Y PREPARACIÓN DE DATOS\")\n","\n","      X_train = [tokens_frequency(d.review,d.vector_cat,Vocabulario) for d in Reviews_train_Over]\n","      svc = SVC(C=2.5, kernel='linear', degree=2, gamma='auto', probability=True).fit(X_train,Label_train_over)\n","\n","      # svc = SVC(kernel='linear', C=2.5, decision_function_shape='ovo').fit(X_train,Label_train_over)\n","      # svc = SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train,Label_train_)\n","\n","      X_test = [tokens_frequency(d.review,d.vector_cat,Vocabulario) for d in Reviews_test_]\n","\n","\n","      pred = svc.predict(X_test)\n","\n","      # Datos de predicción\n","\n","      probabilidades = svc.predict_proba(X_test)\n","      for i in range(len(x_test)):\n","        if polaridad == 'negative':\n","          PReviews[i].prob_neg = probabilidades[i][1]\n","        else:\n","          PReviews[i].prob_pos = probabilidades[i][1]\n","\n","\n","\n","      y__test = Label_test_\n","      # print(confusion_matrix(Label_test_, pred))\n","      # print('accuracy score %0.3f' % svc.score(X_test, y__test))\n","      # print(metrics.classification_report(y__test,pred) )\n","\n","  PCategory_.Reviews = PReviews\n","  return PCategory_\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMMYGF6YhwxX"},"source":["CNN\n","---"]},{"cell_type":"code","metadata":{"id":"Lvo2OrbdhyS6"},"source":["import re\n","import pandas\n","from collections import defaultdict\n","from pandas import DataFrame\n","\n","def CNN_oversampling_binario(categoria, x_train, y_train, x_test, y_test):\n","\n","  print(categoria.category)\n","\n","  ##############################################################\n","  #Datos de clasificación\n","  PCategory_ = PCategory(categoria.category)\n","\n","  PReviews = []\n","  contador = 0\n","  for review in x_test:\n","    PReview_ = PReview(review,contador,y_test[contador])\n","    PReviews.append(PReview_)\n","    contador += 1\n","\n","\n","  polaridades = [\"negative\",\"positive\"] #No se debe considerar la polaridad NEUTRAL\n","\n","  for polaridad in polaridades:\n","    print(polaridad)\n","    # Agrupar información de reviews en un solo conjunto de datos\n","    Reviews_train_ = []\n","    Reviews_train_1 = []\n","    Label_train_ = []\n","    contador = 0\n","    flag = 0\n","    positivos = 0\n","    negativos = 0\n","\n","\n","    if polaridad == 'negative':\n","      vec_categoria = categoria.negativos.vector\n","    else:\n","      vec_categoria = categoria.positivos.vector\n","\n","     # print(y_train)\n","\n","    for doc in x_train:\n","      if y_train[contador] != \"neutral\" and y_train[contador] != \"conflict\":\n","      # if y_train[contador] != \"conflict\":\n","        if y_train[contador] == polaridad:\n","          Reviews_train_1.append(doc)\n","          Label_train_.append(1)\n","        else:\n","          Label_train_.append(0)\n","\n","        Reviews_train_.append(doc)\n","\n","      contador += 1\n","\n","\n","    if len(Reviews_train_) > 0:\n","      Vocabulario = Vocabulario_train_Over(Reviews_train_1)\n","\n","      ###############################################################\n","      # Oversampling strategy - 21/03/21\n","      # instantiating the random over sampler\n","      ros = RandomOverSampler()\n","\n","      # resampling X, y\n","      X = DataFrame(Reviews_train_,columns=['review'])\n","\n","      X_ros, y_ros = ros.fit_resample(X, Label_train_)\n","\n","      # new class distribution\n","      # print(Counter(y_ros))\n","      # print(y_ros)\n","\n","      Reviews_train_Over = []\n","      Label_train_over = []\n","\n","      for i in range(len(X_ros)):\n","        # review = Review(X_ros[i][0])\n","        # review.vector_cat = vec_categoria\n","        Label_train_over.append(y_ros[i])\n","        Reviews_train_Over.append(X_ros[i][0])\n","\n","      # print(Reviews_train_Over)\n","\n","      ################################################################\n","\n","      # Max-Lenght\n","      max_length = max([len(s.split()) for s in Reviews_train_Over])\n","      # ################################################################\n","\n","\n","      # Datos de test\n","      Reviews_test_ = []\n","      Label_test_ = []\n","      contador = 0\n","\n","      for doc in x_test:\n","        if y_test[contador] != \"neutral\" and y_test[contador] != \"conflict\":\n","        # if y_test[contador] != \"conflict\":\n","          # review = Review(doc)\n","          # review.polaridad = y_test[contador].lower()\n","          # review.vector_cat = vec_categoria\n","\n","          if y_test[contador] == polaridad:\n","            Label_test_.append(1)\n","          else:\n","            Label_test_.append(0)\n","\n","          Reviews_test_.append(doc)\n","        contador += 1\n","\n","\n","\n","      embedding_dim = 200\n","      vocabulary_size = len(Vocabulario) #len(Vocabulario) #20000\n","      tokenizer = Tokenizer(num_words = vocabulary_size)\n","      tokenizer.fit_on_texts(Reviews_train_1)\n","      sequences = tokenizer.texts_to_sequences(Reviews_train_Over)\n","      data_train = pad_sequences(sequences, maxlen=max_length)\n","\n","      tokenizer = Tokenizer(num_words = vocabulary_size)\n","      tokenizer.fit_on_texts(Reviews_train_1)\n","\n","      sequences = tokenizer.texts_to_sequences(Reviews_test_)\n","      data_test = pad_sequences(sequences, maxlen = max_length)\n","\n","\n","      #Embedding Matrix\n","      embeddings_index = dict()\n","      for word in model.index2word:\n","        embeddings_index[word] = np.asarray(model[word],dtype='float32')\n","\n","\n","      print('Loaded %s word vectors.' %len(embeddings_index))\n","\n","      # create a weight matrix for words in training reviews\n","      embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n","      for word, index in tokenizer.word_index.items():\n","        if index > vocabulary_size - 1:\n","          break\n","        else:\n","          embedding_vector = embeddings_index.get(word)\n","          if embedding_vector is not None:\n","            # wordVector = VectorRelation(word,embedding_vector)\n","            cosine = cos_sim(embedding_vector,vec_categoria)\n","            wordVector = [n * cosine for n in embedding_vector]\n","            embedding_matrix[index] =  wordVector #embedding_vector\n","\n","      # print(embedding_matrix)\n","\n","      # model_glove = Sequential()\n","      # model_glove.add(Embedding(vocabulary_size,embedding_dim,input_length=max_length,weights=[embedding_matrix],trainable=False))\n","\n","      #--------------------------------------------------\n","      \"\"\"https://unipython.com/proyecto-desarrollar-un-modelo-de-incrustacion-cnn-para-el-analisis-de-sentimientos/\"\"\"\n","      #configuración CNN\n","\n","      # model_glove.add(Conv1D(filters=256,kernel_size=3,activation='relu'))\n","      # model_glove.add(MaxPooling1D(pool_size=3))\n","\n","      # model_glove.add(Flatten())\n","      # # model_glove.add(Dense(10, activation=\"relu\"))\n","      # model_glove.add(Dense(1, activation=\"sigmoid\"))\n","\n","\n","      #------------------------------------------------\n","\n","            #--------------------------------------------------\n","      #03-05-2021\n","      #CNN con multiples kernels (capas a mismo nivel de convolución)\n","      num_filters = 256\n","\n","      inputs_2 = Input(shape=(max_length,), dtype='int32')\n","      embedding_layer_2 = Embedding(vocabulary_size,embedding_dim, embeddings_initializer=Constant(embedding_matrix),input_length=max_length, trainable=False)(inputs_2)\n","\n","      conv_0 = Conv1D(num_filters, kernel_size=1, activation='relu')(embedding_layer_2)\n","      conv_1 = Conv1D(num_filters, kernel_size=2, activation='relu')(embedding_layer_2)\n","      conv_2 = Conv1D(num_filters, kernel_size=3, activation='relu')(embedding_layer_2)\n","\n","      maxpool_0 = MaxPool1D(pool_size=(max_length - 1 + 1), strides=1, padding='valid')(conv_0)\n","      maxpool_1 = MaxPool1D(pool_size=(max_length - 2 + 1), strides=1, padding='valid')(conv_1)\n","      maxpool_2 = MaxPool1D(pool_size=(max_length - 3 + 1), strides=1, padding='valid')(conv_2)\n","\n","      concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n","      flatten_2 = Flatten()(concatenated_tensor)\n","\n","      dropout_2 = Dropout(0.5)(flatten_2)\n","      output_2 = Dense(units=1, activation='sigmoid')(dropout_2)\n","\n","      # model_glove.add(Dense(10, activation=\"relu\"))\n","      # model_glove.add(Dense(1, activation=\"sigmoid\"))\n","      #------------------------------------------------\n","\n","      #------------------------------------------------\n","      \"\"\"\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\"\"\"\n","      # #Configruacion BiLSTM\n","      # forward_layer = LSTM(10, return_sequences=True)\n","      # backward_layer = LSTM(10, activation='relu',return_sequences=True, go_backwards=True)\n","      # model_glove.add(Bidirectional(forward_layer,backward_layer=backward_layer, input_shape=(5,10)))\n","      # model_glove.add(Flatten())\n","      # model_glove.add(Dense(1, activation=\"sigmoid\"))\n","      # # model_glove.add(Dense(5))\n","      # model_glove.add(Activation('softmax'))\n","\n","      #-----------------------------------------------------------------------\n","\n","\n","      # model_glove.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","      model_2 = Model(inputs=inputs_2, outputs=output_2)\n","      model_2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","      # model_glove.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy']) #for CNN-BiLSTM\n","\n","      # model_glove.fit(data_train,np.array(Label_train_over),validation_split=0.4,epochs=9)\n","      # model_glove.fit(data_train,np.array(Label_train_over),validation_split=0.4,epochs=11)\n","      batch_size = 64\n","      model_2.fit(data_train,np.array(Label_train_over), epochs=9, batch_size=batch_size, verbose=1, validation_split=0.2)\n","\n","\n","\n","\n","      scores = model_2.evaluate(data_test,np.array(Label_test_), verbose=0)\n","      print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","      y_pred = model_2.predict(data_test,batch_size=64,verbose=1)\n","\n","      #Almacenar información de predicción\n","      for i in range(len(x_test)):\n","        if polaridad == 'negative':\n","          PReviews[i].prob_neg = y_pred[i]\n","        else:\n","          PReviews[i].prob_pos = y_pred[i]\n","\n","      # print(y_pred)\n","      y_pred = (y_pred>0.2)\n","      # print(y_pred)\n","      list(y_pred)\n","\n","\n","\n","\n","      cm = confusion_matrix(np.array(Label_test_), y_pred)\n","      print(cm)\n","\n","      print(classification_report(np.array(Label_test_),y_pred))\n","\n","  PCategory_.Reviews = PReviews\n","  return PCategory_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-fovVeZh1BR"},"source":["import re\n","import pandas\n","from collections import defaultdict\n","from pandas import DataFrame\n","\n","def CNN_binario(categoria, x_train, y_train, x_test, y_test):\n","\n","  print(categoria.category)\n","\n","  ##############################################################\n","  #Datos de clasificación\n","  PCategory_ = PCategory(categoria.category)\n","\n","  PReviews = []\n","  contador = 0\n","  for review in x_test:\n","    PReview_ = PReview(review,contador,y_test[contador])\n","    PReviews.append(PReview_)\n","    contador += 1\n","\n","\n","  polaridades = [\"negative\",\"positive\"] #No se debe considerar la polaridad NEUTRAL\n","\n","  for polaridad in polaridades:\n","    print(polaridad)\n","    # Agrupar información de reviews en un solo conjunto de datos\n","    Reviews_train_ = []\n","    Reviews_train_1 = []\n","    Label_train_ = []\n","    contador = 0\n","    flag = 0\n","    positivos = 0\n","    negativos = 0\n","\n","\n","    if polaridad == 'negative':\n","      vec_categoria = categoria.negativos.vector\n","    else:\n","      vec_categoria = categoria.positivos.vector\n","\n","     # print(y_train)\n","\n","    for doc in x_train:\n","      if y_train[contador] != \"neutral\" and y_train[contador] != \"conflict\":\n","      # if y_train[contador] != \"conflict\":\n","        if y_train[contador] == polaridad:\n","          Reviews_train_1.append(doc)\n","          Label_train_.append(1)\n","        else:\n","          Label_train_.append(0)\n","\n","        Reviews_train_.append(doc)\n","\n","      contador += 1\n","\n","\n","    if len(Reviews_train_) > 0:\n","      Vocabulario = Vocabulario_train_Over(Reviews_train_1)\n","\n","\n","      # Max-Lenght\n","      max_length = max([len(s.split()) for s in Reviews_train_])\n","      # ################################################################\n","\n","\n","      # Datos de test\n","      Reviews_test_ = []\n","      Label_test_ = []\n","      contador = 0\n","\n","      for doc in x_test:\n","        if y_test[contador] != \"neutral\" and y_test[contador] != \"conflict\":\n","        # if y_test[contador] != \"conflict\":\n","          # review = Review(doc)\n","          # review.polaridad = y_test[contador].lower()\n","          # review.vector_cat = vec_categoria\n","\n","          if y_test[contador] == polaridad:\n","            Label_test_.append(1)\n","          else:\n","            Label_test_.append(0)\n","\n","          Reviews_test_.append(doc)\n","        contador += 1\n","\n","\n","\n","\n","      embedding_dim = 200\n","      vocabulary_size = len(Vocabulario) #len(Vocabulario) #20000\n","      tokenizer = Tokenizer(num_words = vocabulary_size)\n","      tokenizer.fit_on_texts(Reviews_train_1)\n","      sequences = tokenizer.texts_to_sequences(Reviews_train_)\n","      data_train = pad_sequences(sequences, maxlen=max_length)\n","\n","      tokenizer = Tokenizer(num_words = vocabulary_size)\n","      tokenizer.fit_on_texts(Reviews_train_1)\n","\n","      sequences = tokenizer.texts_to_sequences(Reviews_test_)\n","      data_test = pad_sequences(sequences, maxlen = max_length)\n","\n","\n","      #Embedding Matrix\n","      embeddings_index = dict()\n","      for word in model.index2word:\n","        embeddings_index[word] = np.asarray(model[word],dtype='float32')\n","\n","\n","      print('Loaded %s word vectors.' %len(embeddings_index))\n","\n","      # create a weight matrix for words in training reviews\n","      embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n","      for word, index in tokenizer.word_index.items():\n","        if index > vocabulary_size - 1:\n","          break\n","        else:\n","          embedding_vector = embeddings_index.get(word)\n","          if embedding_vector is not None:\n","            # wordVector = VectorRelation(word,embedding_vector)\n","            cosine = cos_sim(embedding_vector,vec_categoria)\n","            wordVector = [n * cosine for n in embedding_vector]\n","            embedding_matrix[index] =  wordVector #embedding_vector\n","\n","      # print(embedding_matrix)\n","\n","      # model_glove = Sequential()\n","      # model_glove.add(Embedding(vocabulary_size,300,input_length=max_length,weights=[embedding_matrix],trainable=False))\n","\n","      #--------------------------------------------------\n","      \"\"\"https://unipython.com/proyecto-desarrollar-un-modelo-de-incrustacion-cnn-para-el-analisis-de-sentimientos/\"\"\"\n","  #     #configuración CNN\n","\n","  #     # model_glove.add(Conv1D(filters=256,kernel_size=3,activation='relu'))\n","  #     # model_glove.add(MaxPooling1D(pool_size=3))\n","\n","  #     # model_glove.add(Flatten())\n","  #     # # model_glove.add(Dense(10, activation=\"relu\"))\n","  #     # model_glove.add(Dense(1, activation=\"sigmoid\"))\n","\n","\n","  #     #------------------------------------------------\n","\n","            #--------------------------------------------------\n","      #03-05-2021\n","      #CNN con multiples kernels (capas a mismo nivel de convolución)\n","      num_filters = 256\n","\n","      inputs_2 = Input(shape=(max_length,), dtype='int32')\n","      embedding_layer_2 = Embedding(vocabulary_size,embedding_dim, embeddings_initializer=Constant(embedding_matrix),input_length=max_length, trainable=False)(inputs_2)\n","\n","      conv_0 = Conv1D(num_filters, kernel_size=1, activation='relu')(embedding_layer_2)\n","      conv_1 = Conv1D(num_filters, kernel_size=2, activation='relu')(embedding_layer_2)\n","      conv_2 = Conv1D(num_filters, kernel_size=3, activation='relu')(embedding_layer_2)\n","\n","      maxpool_0 = MaxPool1D(pool_size=(max_length - 1 + 1), strides=1, padding='valid')(conv_0)\n","      maxpool_1 = MaxPool1D(pool_size=(max_length - 2 + 1), strides=1, padding='valid')(conv_1)\n","      maxpool_2 = MaxPool1D(pool_size=(max_length - 3 + 1), strides=1, padding='valid')(conv_2)\n","\n","      concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n","      flatten_2 = Flatten()(concatenated_tensor)\n","\n","      dropout_2 = Dropout(0.5)(flatten_2)\n","      output_2 = Dense(units=1, activation='sigmoid')(dropout_2)\n","\n","      # model_glove.add(Dense(10, activation=\"relu\"))\n","      # model_glove.add(Dense(1, activation=\"sigmoid\"))\n","      #------------------------------------------------\n","\n","  #     #------------------------------------------------\n","  #     \"\"\"\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\"\"\"\n","  #     # #Configruacion BiLSTM\n","  #     # forward_layer = LSTM(10, return_sequences=True)\n","  #     # backward_layer = LSTM(10, activation='relu',return_sequences=True, go_backwards=True)\n","  #     # model_glove.add(Bidirectional(forward_layer,backward_layer=backward_layer, input_shape=(5,10)))\n","  #     # model_glove.add(Flatten())\n","  #     # model_glove.add(Dense(1, activation=\"sigmoid\"))\n","  #     # # model_glove.add(Dense(5))\n","  #     # model_glove.add(Activation('softmax'))\n","\n","  #     #-----------------------------------------------------------------------\n","\n","\n","  #     # model_glove.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","      model_2 = Model(inputs=inputs_2, outputs=output_2)\n","      model_2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","  #     # model_glove.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy']) #for CNN-BiLSTM\n","\n","  #     # model_glove.fit(data_train,np.array(Label_train_over),validation_split=0.4,epochs=9)\n","  #     # model_glove.fit(data_train,np.array(Label_train_over),validation_split=0.4,epochs=11)\n","      batch_size = 64\n","      model_2.fit(data_train,np.array(Label_train_), epochs=9, batch_size=batch_size, verbose=1, validation_split=0.2)\n","\n","\n","\n","\n","      scores = model_2.evaluate(data_test,np.array(Label_test_), verbose=0)\n","      print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","      y_pred = model_2.predict(data_test,batch_size=64,verbose=1)\n","\n","      #Almacenar información de predicción\n","      for i in range(len(x_test)):\n","        if polaridad == 'negative':\n","          PReviews[i].prob_neg = y_pred[i]\n","        else:\n","          PReviews[i].prob_pos = y_pred[i]\n","\n","      # print(y_pred)\n","      y_pred = (y_pred>0.2)\n","      # print(y_pred)\n","      list(y_pred)\n","\n","\n","\n","\n","      cm = confusion_matrix(np.array(Label_test_), y_pred)\n","      print(cm)\n","\n","      print(classification_report(np.array(Label_test_),y_pred))\n","\n","  PCategory_.Reviews = PReviews\n","  return PCategory_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zq1Vsjmlh2l2","executionInfo":{"status":"ok","timestamp":1627076165891,"user_tz":300,"elapsed":220539,"user":{"displayName":"Monserrat Vázquez Hernández","photoUrl":"","userId":"12458819110641506711"}},"outputId":"7ea0ff04-0417-4e02-c80b-9d4f0ae22e1e"},"source":["#Listado manual de Entidades - para recorrer directorios y tener léxico jerárquico\n","# categorias_list = [\"AMBIENCE#GENERAL\"]\n","categorias_list = [\"AMBIENCE#GENERAL\", \"DRINKS#QUALITY\",\"DRINKS#STYLE_OPTIONS\",\"DRINKS#PRICES\",\"FOOD#QUALITY\", \"FOOD#STYLE_OPTIONS\",\n","             \"FOOD#PRICES\",\"RESTAURANT#GENERAL\", \"RESTAURANT#MISCELLANEOUS\",\"RESTAURANT#PRICES\",\"SERVICE#GENERAL\",\"LOCATION#GENERAL\"]\n","\n","# categorias_list = [\"AMBIENCE#GENERAL\", \"DRINKS#QUALITY\",\"DRINKS#STYLE_OPTIONS\",\"DRINKS#PRICES\",\"FOOD#QUALITY\", \"FOOD#STYLE_OPTIONS\",\n","#              \"FOOD#PRICES\",\"RESTAURANT#GENERAL\", \"RESTAURANT#MISCELLANEOUS\",\"RESTAURANT#PRICES\",\"SERVICE#GENERAL\"]\n","\n","#Recorrer por categoría:\n","for cat in categorias_list:\n","  print(cat)\n","  # x = 1.46\n","  x = .0\n","\n","  Categoria = Category(cat)\n","  Vocabulario_polaridades_train, x_train, y_train, x_test, y_test = Lexicos_polaridad_categoria(cat)\n","\n","  for item in Vocabulario_polaridades_train:\n","    # print(item.polaridad)\n","    # print(item.vocabulario)\n","\n","    if item.polaridad == \"negative\":\n","      Categoria.negativos.vocabulario = item.vocabulario\n","      # print(\"negative\")\n","      # print(len(item.vocabulario))\n","      Categoria.negativos.vector = VectorUnico(item.vocabulario, item.diccionario_gv, x , item.promedio, item.dev_std, False)\n","    else:\n","        # print(\"positive\")\n","        # print(len(item.vocabulario))\n","        Categoria.positivos.vocabulario = item.vocabulario\n","        Categoria.positivos.vector = VectorUnico(item.vocabulario, item.diccionario_gv, x , item.promedio, item.dev_std, False)\n","\n","\n","  PCategory_ = CNN_oversampling_binario(Categoria, x_train, y_train, x_test, y_test)\n","  # PCategory_ = SVM_OverSampling_binario(Categoria, x_train, y_train, x_test, y_test)\n","\n","  #Análisis de probabiliades\n","  #Resultados\n","  total_reviews = 0\n","  total_correctos = 0\n","\n","  # Resultados erroneos\n","  total_errores = 0\n","  total_errores_neg = 0\n","  total_errores_pos = 0\n","  error_10 = 0\n","  error_20 = 0\n","  error_30 = 0\n","  error_40 = 0\n","  error_50 = 0\n","  error_60 = 0\n","  error_70 = 0\n","  error_80 = 0\n","  error_90 = 0\n","  error_100 = 0\n","\n","  for review in PCategory_.Reviews:\n","    total_reviews += 1\n","\n","\n","    #Clasificación por la probabilidad más alta\n","    diff = 0\n","    if review.prob_neg > review.prob_pos:\n","      review.res_clasificacion = \"negative\"\n","      res_clasificacion = \"negative\"\n","      diff = review.prob_neg - review.prob_pos\n","    else:\n","      review.res_clasificacion = \"positive\"\n","      res_clasificacion = \"positive\"\n","      diff = review.prob_pos - review.prob_neg\n","\n","\n","\n","\n","\n","\n","\n","    # #Clasificación con probabilidad mayor a .5\n","    # if review.prob_neg < 0.3:\n","    #   review.res_clasificacion = \"negative\"\n","    #   res_clasificacion = \"negative\"\n","    # else:\n","    #   review.res_clasificacion = \"positive\"\n","    #   res_clasificacion = \"positive\"\n","\n","\n","    #Total clasificación correcta\n","    if res_clasificacion == review.polaridad_:\n","      total_correctos += 1\n","    else:\n","      total_errores += 1\n","      # print(diff)\n","      if review.polaridad_ == \"negative\":\n","        total_errores_neg += 1\n","      else:\n","        total_errores_pos += 1\n","\n","      if diff <= .10:\n","        error_10 += 1\n","      else:\n","        if diff <= .20:\n","          error_20 += 1\n","        else:\n","          if diff <= .30:\n","            error_30 += 1\n","          else:\n","            if diff <= .40:\n","              error_40 += 1\n","            else:\n","              if diff <= .50:\n","                error_50 += 1\n","              else:\n","                if diff <= .60:\n","                  error_60 += 1\n","                else:\n","                  if diff <= .70:\n","                    error_70 += 1\n","                  else:\n","                    if diff <= .80:\n","                      error_80 += 1\n","                    else:\n","                      if diff <= .90:\n","                        error_90 += 1\n","                      else:\n","                        error_100 += 1\n","\n","      # print(review.review_)\n","      # print(review.polaridad_)\n","      # print(\"NEG:\" + str(review.prob_neg))\n","      # print(\"POS:\" + str(review.prob_pos))\n","      # print(\"--------------\")\n","\n","  # print(\"Total Errores: \" + str(total_errores))\n","  # print(\"Total Errores Neg: \" + str(total_errores_neg))\n","  # print(\"Total Errores Pos: \" + str(total_errores_pos))\n","\n","  # print(\".10: \" + str(error_10))\n","  # print(\".20: \" + str(error_20))\n","  # print(\".30: \" + str(error_30))\n","  # print(\".40: \" + str(error_40))\n","  # print(\".50: \" + str(error_50))\n","  # print(\".60: \" + str(error_60))\n","  # print(\".70: \" + str(error_70))\n","  # print(\".80: \" + str(error_80))\n","  # print(\".90: \" + str(error_90))\n","  # print(\"1.00: \" + str(error_90))\n","\n","\n","  print(total_reviews)\n","  print(total_correctos)\n","  accuracy_ = (total_correctos*100)/total_reviews\n","  print(accuracy_)\n","\n","\n","  # print(Categoria.negativos.polaridad)\n","  # print(Categoria.negativos.vocabulario)\n","  # print(len(Categoria.negativos.vocabulario))\n","  # print(Categoria.negativos.vector)\n","  # print(\"-----------------------------------------\")\n","  # print(Categoria.neutros.polaridad)\n","  # print(Categoria.neutros.vocabulario)\n","  # print(len(Categoria.neutros.vocabulario))\n","  # print(Categoria.neutros.vector)\n","  # print(\"-----------------------------------------\")\n","  # print(Categoria.positivos.polaridad)\n","  # print(Categoria.positivos.vocabulario)\n","  # print(len(Categoria.positivos.vocabulario))\n","  # print(Categoria.positivos.vector)\n","  print(\"--------------------------------------------------------------------------------------------------------\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AMBIENCE#GENERAL\n","AMBIENCE#GENERAL\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 42531.79it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["AMBIENCE#GENERAL\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","5/5 [==============================] - 2s 167ms/step - loss: 0.5688 - accuracy: 0.7073 - val_loss: 0.3538 - val_accuracy: 0.8228\n","Epoch 2/9\n","5/5 [==============================] - 0s 44ms/step - loss: 0.2785 - accuracy: 0.9124 - val_loss: 0.1297 - val_accuracy: 0.9873\n","Epoch 3/9\n","5/5 [==============================] - 0s 46ms/step - loss: 0.1752 - accuracy: 0.9655 - val_loss: 0.1016 - val_accuracy: 0.9873\n","Epoch 4/9\n","5/5 [==============================] - 0s 43ms/step - loss: 0.1290 - accuracy: 0.9893 - val_loss: 0.0848 - val_accuracy: 0.9873\n","Epoch 5/9\n","5/5 [==============================] - 0s 47ms/step - loss: 0.0909 - accuracy: 0.9951 - val_loss: 0.0547 - val_accuracy: 0.9873\n","Epoch 6/9\n","5/5 [==============================] - 0s 50ms/step - loss: 0.0838 - accuracy: 0.9961 - val_loss: 0.0348 - val_accuracy: 0.9873\n","Epoch 7/9\n","5/5 [==============================] - 0s 46ms/step - loss: 0.0743 - accuracy: 0.9983 - val_loss: 0.0277 - val_accuracy: 0.9873\n","Epoch 8/9\n","5/5 [==============================] - 0s 49ms/step - loss: 0.0572 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n","Epoch 9/9\n","5/5 [==============================] - 0s 47ms/step - loss: 0.0527 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n","Accuracy: 96.83%\n","1/1 [==============================] - 0s 324ms/step\n","[[50 11]\n"," [ 1  1]]\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.82      0.89        61\n","           1       0.08      0.50      0.14         2\n","\n","    accuracy                           0.81        63\n","   macro avg       0.53      0.66      0.52        63\n","weighted avg       0.95      0.81      0.87        63\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","5/5 [==============================] - 2s 160ms/step - loss: 0.5981 - accuracy: 0.6885 - val_loss: 0.7212 - val_accuracy: 0.4684\n","Epoch 2/9\n","5/5 [==============================] - 0s 44ms/step - loss: 0.3786 - accuracy: 0.8641 - val_loss: 0.3226 - val_accuracy: 0.9747\n","Epoch 3/9\n","5/5 [==============================] - 0s 48ms/step - loss: 0.2479 - accuracy: 0.9426 - val_loss: 0.3626 - val_accuracy: 0.8481\n","Epoch 4/9\n","5/5 [==============================] - 0s 48ms/step - loss: 0.1874 - accuracy: 0.9369 - val_loss: 0.2043 - val_accuracy: 1.0000\n","Epoch 5/9\n","5/5 [==============================] - 0s 48ms/step - loss: 0.1345 - accuracy: 0.9737 - val_loss: 0.1281 - val_accuracy: 1.0000\n","Epoch 6/9\n","5/5 [==============================] - 0s 44ms/step - loss: 0.1038 - accuracy: 0.9782 - val_loss: 0.1274 - val_accuracy: 1.0000\n","Epoch 7/9\n","5/5 [==============================] - 0s 52ms/step - loss: 0.0838 - accuracy: 0.9880 - val_loss: 0.1153 - val_accuracy: 1.0000\n","Epoch 8/9\n","5/5 [==============================] - 0s 45ms/step - loss: 0.0645 - accuracy: 0.9776 - val_loss: 0.0843 - val_accuracy: 1.0000\n","Epoch 9/9\n","5/5 [==============================] - 0s 45ms/step - loss: 0.0597 - accuracy: 0.9854 - val_loss: 0.0727 - val_accuracy: 1.0000\n","Accuracy: 85.71%\n","1/1 [==============================] - 0s 337ms/step\n","[[ 0  2]\n"," [ 4 57]]\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         2\n","           1       0.97      0.93      0.95        61\n","\n","    accuracy                           0.90        63\n","   macro avg       0.48      0.47      0.47        63\n","weighted avg       0.94      0.90      0.92        63\n","\n","63\n","61\n","96.82539682539682\n","--------------------------------------------------------------------------------------------------------\n","DRINKS#QUALITY\n","DRINKS#QUALITY\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 54731.45it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["DRINKS#QUALITY\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.7103 - accuracy: 0.7344 - val_loss: 0.1742 - val_accuracy: 1.0000\n","Epoch 2/9\n","1/1 [==============================] - 0s 70ms/step - loss: 0.4971 - accuracy: 0.5156 - val_loss: 0.0681 - val_accuracy: 1.0000\n","Epoch 3/9\n","1/1 [==============================] - 0s 54ms/step - loss: 0.4526 - accuracy: 0.5156 - val_loss: 0.0371 - val_accuracy: 1.0000\n","Epoch 4/9\n","1/1 [==============================] - 0s 71ms/step - loss: 0.3847 - accuracy: 0.9531 - val_loss: 0.0248 - val_accuracy: 1.0000\n","Epoch 5/9\n","1/1 [==============================] - 0s 51ms/step - loss: 0.3606 - accuracy: 0.9688 - val_loss: 0.0184 - val_accuracy: 1.0000\n","Epoch 6/9\n","1/1 [==============================] - 0s 89ms/step - loss: 0.3423 - accuracy: 0.9688 - val_loss: 0.0150 - val_accuracy: 1.0000\n","Epoch 7/9\n","1/1 [==============================] - 0s 53ms/step - loss: 0.3179 - accuracy: 0.9688 - val_loss: 0.0127 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3051 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3121 - accuracy: 0.9844 - val_loss: 0.0095 - val_accuracy: 1.0000\n","Accuracy: 90.91%\n","1/1 [==============================] - 0s 344ms/step\n","[[ 3 18]\n"," [ 0  1]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.14      0.25        21\n","           1       0.05      1.00      0.10         1\n","\n","    accuracy                           0.18        22\n","   macro avg       0.53      0.57      0.17        22\n","weighted avg       0.96      0.18      0.24        22\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.7448 - accuracy: 0.5625 - val_loss: 0.6734 - val_accuracy: 0.5625\n","Epoch 2/9\n","1/1 [==============================] - 0s 51ms/step - loss: 0.4704 - accuracy: 0.7812 - val_loss: 0.6920 - val_accuracy: 0.5625\n","Epoch 3/9\n","1/1 [==============================] - 0s 48ms/step - loss: 0.3062 - accuracy: 0.8750 - val_loss: 0.5700 - val_accuracy: 0.8750\n","Epoch 4/9\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2446 - accuracy: 0.9219 - val_loss: 0.4273 - val_accuracy: 0.8750\n","Epoch 5/9\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1799 - accuracy: 0.9375 - val_loss: 0.3128 - val_accuracy: 0.8750\n","Epoch 6/9\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1230 - accuracy: 0.9844 - val_loss: 0.2322 - val_accuracy: 0.8750\n","Epoch 7/9\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1011 - accuracy: 0.9688 - val_loss: 0.1763 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0828 - accuracy: 0.9844 - val_loss: 0.1380 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0830 - accuracy: 0.9844 - val_loss: 0.1110 - val_accuracy: 1.0000\n","Accuracy: 90.91%\n","1/1 [==============================] - 0s 314ms/step\n","[[ 0  1]\n"," [ 1 20]]\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         1\n","           1       0.95      0.95      0.95        21\n","\n","    accuracy                           0.91        22\n","   macro avg       0.48      0.48      0.48        22\n","weighted avg       0.91      0.91      0.91        22\n","\n","22\n","20\n","90.9090909090909\n","--------------------------------------------------------------------------------------------------------\n","DRINKS#STYLE_OPTIONS\n","DRINKS#STYLE_OPTIONS\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 43470.55it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["DRINKS#STYLE_OPTIONS\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 3s 3s/step - loss: 0.6309 - accuracy: 0.8261 - val_loss: 0.0873 - val_accuracy: 1.0000\n","Epoch 2/9\n","1/1 [==============================] - 0s 53ms/step - loss: 0.5148 - accuracy: 0.3913 - val_loss: 0.0403 - val_accuracy: 1.0000\n","Epoch 3/9\n","1/1 [==============================] - 0s 52ms/step - loss: 0.4303 - accuracy: 0.6087 - val_loss: 0.0308 - val_accuracy: 1.0000\n","Epoch 4/9\n","1/1 [==============================] - 0s 55ms/step - loss: 0.3824 - accuracy: 0.9565 - val_loss: 0.0297 - val_accuracy: 1.0000\n","Epoch 5/9\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3656 - accuracy: 0.9565 - val_loss: 0.0316 - val_accuracy: 1.0000\n","Epoch 6/9\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3491 - accuracy: 0.9783 - val_loss: 0.0328 - val_accuracy: 1.0000\n","Epoch 7/9\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3102 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3084 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3033 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n","Accuracy: 91.67%\n","1/1 [==============================] - 0s 344ms/step\n","[[4 7]\n"," [0 1]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.36      0.53        11\n","           1       0.12      1.00      0.22         1\n","\n","    accuracy                           0.42        12\n","   macro avg       0.56      0.68      0.38        12\n","weighted avg       0.93      0.42      0.51        12\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.7190 - accuracy: 0.5000 - val_loss: 0.7561 - val_accuracy: 0.3333\n","Epoch 2/9\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3798 - accuracy: 0.9130 - val_loss: 0.6807 - val_accuracy: 0.6667\n","Epoch 3/9\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2666 - accuracy: 0.9130 - val_loss: 0.4785 - val_accuracy: 0.6667\n","Epoch 4/9\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1764 - accuracy: 0.9348 - val_loss: 0.2949 - val_accuracy: 1.0000\n","Epoch 5/9\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1300 - accuracy: 0.9565 - val_loss: 0.1605 - val_accuracy: 1.0000\n","Epoch 6/9\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 1.0000\n","Epoch 7/9\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 1.0000\n","Accuracy: 83.33%\n","1/1 [==============================] - 0s 324ms/step\n","[[ 0  1]\n"," [ 0 11]]\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         1\n","           1       0.92      1.00      0.96        11\n","\n","    accuracy                           0.92        12\n","   macro avg       0.46      0.50      0.48        12\n","weighted avg       0.84      0.92      0.88        12\n","\n","12\n","10\n","83.33333333333333\n","--------------------------------------------------------------------------------------------------------\n","DRINKS#PRICES\n","DRINKS#PRICES\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 53725.30it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["DRINKS#PRICES\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.8194 - accuracy: 0.6000 - val_loss: 0.6654 - val_accuracy: 0.3333\n","Epoch 2/9\n","1/1 [==============================] - 0s 47ms/step - loss: 0.4678 - accuracy: 0.7500 - val_loss: 0.4664 - val_accuracy: 1.0000\n","Epoch 3/9\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3114 - accuracy: 1.0000 - val_loss: 0.3197 - val_accuracy: 1.0000\n","Epoch 4/9\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2459 - accuracy: 1.0000 - val_loss: 0.2188 - val_accuracy: 1.0000\n","Epoch 5/9\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2052 - accuracy: 1.0000 - val_loss: 0.1349 - val_accuracy: 1.0000\n","Epoch 6/9\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1973 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 1.0000\n","Epoch 7/9\n","1/1 [==============================] - 0s 41ms/step - loss: 0.1448 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 36ms/step - loss: 0.1310 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 41ms/step - loss: 0.1345 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 1.0000\n","Accuracy: 0.00%\n","1/1 [==============================] - 0s 304ms/step\n","[[4]]\n","              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00         4\n","\n","    accuracy                           1.00         4\n","   macro avg       1.00      1.00      1.00         4\n","weighted avg       1.00      1.00      1.00         4\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.5869 - accuracy: 0.6000 - val_loss: 0.7856 - val_accuracy: 0.0000e+00\n","Epoch 2/9\n","1/1 [==============================] - 0s 34ms/step - loss: 0.4111 - accuracy: 0.7000 - val_loss: 0.6134 - val_accuracy: 0.6667\n","Epoch 3/9\n","1/1 [==============================] - 0s 41ms/step - loss: 0.3353 - accuracy: 0.9000 - val_loss: 0.4568 - val_accuracy: 1.0000\n","Epoch 4/9\n","1/1 [==============================] - 0s 40ms/step - loss: 0.2051 - accuracy: 0.9500 - val_loss: 0.3409 - val_accuracy: 1.0000\n","Epoch 5/9\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1746 - accuracy: 0.9500 - val_loss: 0.2622 - val_accuracy: 1.0000\n","Epoch 6/9\n","1/1 [==============================] - 0s 35ms/step - loss: 0.1277 - accuracy: 1.0000 - val_loss: 0.2110 - val_accuracy: 1.0000\n","Epoch 7/9\n","1/1 [==============================] - 0s 39ms/step - loss: 0.1022 - accuracy: 1.0000 - val_loss: 0.1800 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0799 - accuracy: 1.0000 - val_loss: 0.1599 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0727 - accuracy: 1.0000 - val_loss: 0.1474 - val_accuracy: 1.0000\n","Accuracy: 75.00%\n","1/1 [==============================] - 0s 321ms/step\n","[[0 4]\n"," [0 0]]\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       4.0\n","           1       0.00      0.00      0.00       0.0\n","\n","    accuracy                           0.00       4.0\n","   macro avg       0.00      0.00      0.00       4.0\n","weighted avg       0.00      0.00      0.00       4.0\n","\n","4\n","0\n","0.0\n","--------------------------------------------------------------------------------------------------------\n","FOOD#QUALITY\n","FOOD#QUALITY\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 50655.28it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["FOOD#QUALITY\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","16/16 [==============================] - 3s 78ms/step - loss: 0.5735 - accuracy: 0.7063 - val_loss: 0.3186 - val_accuracy: 0.8907\n","Epoch 2/9\n","16/16 [==============================] - 1s 48ms/step - loss: 0.2806 - accuracy: 0.8844 - val_loss: 0.2850 - val_accuracy: 0.8988\n","Epoch 3/9\n","16/16 [==============================] - 1s 50ms/step - loss: 0.1771 - accuracy: 0.9531 - val_loss: 0.2534 - val_accuracy: 0.9150\n","Epoch 4/9\n","16/16 [==============================] - 1s 50ms/step - loss: 0.1556 - accuracy: 0.9676 - val_loss: 0.1938 - val_accuracy: 0.9352\n","Epoch 5/9\n","16/16 [==============================] - 1s 49ms/step - loss: 0.1318 - accuracy: 0.9657 - val_loss: 0.1046 - val_accuracy: 0.9717\n","Epoch 6/9\n","16/16 [==============================] - 1s 47ms/step - loss: 0.1130 - accuracy: 0.9723 - val_loss: 0.1187 - val_accuracy: 0.9676\n","Epoch 7/9\n","16/16 [==============================] - 1s 48ms/step - loss: 0.1009 - accuracy: 0.9822 - val_loss: 0.1635 - val_accuracy: 0.9636\n","Epoch 8/9\n","16/16 [==============================] - 1s 50ms/step - loss: 0.0773 - accuracy: 0.9899 - val_loss: 0.1142 - val_accuracy: 0.9676\n","Epoch 9/9\n","16/16 [==============================] - 1s 49ms/step - loss: 0.0741 - accuracy: 0.9823 - val_loss: 0.0790 - val_accuracy: 0.9798\n","Accuracy: 89.33%\n","5/5 [==============================] - 0s 15ms/step\n","[[217  52]\n"," [ 13  18]]\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.81      0.87       269\n","           1       0.26      0.58      0.36        31\n","\n","    accuracy                           0.78       300\n","   macro avg       0.60      0.69      0.61       300\n","weighted avg       0.87      0.78      0.82       300\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","16/16 [==============================] - 3s 82ms/step - loss: 0.7074 - accuracy: 0.5987 - val_loss: 0.9720 - val_accuracy: 0.3198\n","Epoch 2/9\n","16/16 [==============================] - 1s 50ms/step - loss: 0.3929 - accuracy: 0.8265 - val_loss: 0.5809 - val_accuracy: 0.7935\n","Epoch 3/9\n","16/16 [==============================] - 1s 47ms/step - loss: 0.2711 - accuracy: 0.9147 - val_loss: 0.4154 - val_accuracy: 0.9028\n","Epoch 4/9\n","16/16 [==============================] - 1s 49ms/step - loss: 0.2028 - accuracy: 0.9433 - val_loss: 0.2841 - val_accuracy: 0.9393\n","Epoch 5/9\n","16/16 [==============================] - 1s 49ms/step - loss: 0.1550 - accuracy: 0.9673 - val_loss: 0.2100 - val_accuracy: 0.9555\n","Epoch 6/9\n","16/16 [==============================] - 1s 48ms/step - loss: 0.1450 - accuracy: 0.9648 - val_loss: 0.2201 - val_accuracy: 0.9595\n","Epoch 7/9\n","16/16 [==============================] - 1s 46ms/step - loss: 0.1134 - accuracy: 0.9776 - val_loss: 0.2075 - val_accuracy: 0.9555\n","Epoch 8/9\n","16/16 [==============================] - 1s 47ms/step - loss: 0.1215 - accuracy: 0.9710 - val_loss: 0.1765 - val_accuracy: 0.9555\n","Epoch 9/9\n","16/16 [==============================] - 1s 50ms/step - loss: 0.0933 - accuracy: 0.9804 - val_loss: 0.1118 - val_accuracy: 0.9919\n","Accuracy: 72.33%\n","5/5 [==============================] - 0s 18ms/step\n","[[ 14  17]\n"," [ 34 235]]\n","              precision    recall  f1-score   support\n","\n","           0       0.29      0.45      0.35        31\n","           1       0.93      0.87      0.90       269\n","\n","    accuracy                           0.83       300\n","   macro avg       0.61      0.66      0.63       300\n","weighted avg       0.87      0.83      0.85       300\n","\n","300\n","270\n","90.0\n","--------------------------------------------------------------------------------------------------------\n","FOOD#STYLE_OPTIONS\n","FOOD#STYLE_OPTIONS\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 47323.95it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["FOOD#STYLE_OPTIONS\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","3/3 [==============================] - 2s 274ms/step - loss: 0.7642 - accuracy: 0.5751 - val_loss: 0.1539 - val_accuracy: 1.0000\n","Epoch 2/9\n","3/3 [==============================] - 0s 31ms/step - loss: 0.5272 - accuracy: 0.7288 - val_loss: 0.1384 - val_accuracy: 1.0000\n","Epoch 3/9\n","3/3 [==============================] - 0s 34ms/step - loss: 0.3447 - accuracy: 0.9270 - val_loss: 0.2203 - val_accuracy: 1.0000\n","Epoch 4/9\n","3/3 [==============================] - 0s 35ms/step - loss: 0.2919 - accuracy: 0.9635 - val_loss: 0.2126 - val_accuracy: 1.0000\n","Epoch 5/9\n","3/3 [==============================] - 0s 33ms/step - loss: 0.2318 - accuracy: 0.9846 - val_loss: 0.1553 - val_accuracy: 1.0000\n","Epoch 6/9\n","3/3 [==============================] - 0s 33ms/step - loss: 0.2028 - accuracy: 0.9789 - val_loss: 0.1110 - val_accuracy: 1.0000\n","Epoch 7/9\n","3/3 [==============================] - 0s 33ms/step - loss: 0.1889 - accuracy: 0.9789 - val_loss: 0.0863 - val_accuracy: 1.0000\n","Epoch 8/9\n","3/3 [==============================] - 0s 34ms/step - loss: 0.1657 - accuracy: 0.9692 - val_loss: 0.0626 - val_accuracy: 1.0000\n","Epoch 9/9\n","3/3 [==============================] - 0s 32ms/step - loss: 0.1649 - accuracy: 0.9770 - val_loss: 0.0505 - val_accuracy: 1.0000\n","Accuracy: 65.22%\n","1/1 [==============================] - 0s 343ms/step\n","[[17 14]\n"," [ 7  8]]\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.55      0.62        31\n","           1       0.36      0.53      0.43        15\n","\n","    accuracy                           0.54        46\n","   macro avg       0.54      0.54      0.53        46\n","weighted avg       0.60      0.54      0.56        46\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","3/3 [==============================] - 2s 267ms/step - loss: 0.6250 - accuracy: 0.6888 - val_loss: 1.0863 - val_accuracy: 0.0882\n","Epoch 2/9\n","3/3 [==============================] - 0s 31ms/step - loss: 0.4606 - accuracy: 0.7192 - val_loss: 0.6706 - val_accuracy: 0.5882\n","Epoch 3/9\n","3/3 [==============================] - 0s 32ms/step - loss: 0.3649 - accuracy: 0.8422 - val_loss: 0.5049 - val_accuracy: 0.8529\n","Epoch 4/9\n","3/3 [==============================] - 0s 36ms/step - loss: 0.3080 - accuracy: 0.9097 - val_loss: 0.4762 - val_accuracy: 0.8529\n","Epoch 5/9\n","3/3 [==============================] - 0s 42ms/step - loss: 0.2694 - accuracy: 0.9288 - val_loss: 0.4414 - val_accuracy: 0.8529\n","Epoch 6/9\n","3/3 [==============================] - 0s 33ms/step - loss: 0.2231 - accuracy: 0.9443 - val_loss: 0.3430 - val_accuracy: 1.0000\n","Epoch 7/9\n","3/3 [==============================] - 0s 34ms/step - loss: 0.1991 - accuracy: 0.9635 - val_loss: 0.2714 - val_accuracy: 1.0000\n","Epoch 8/9\n","3/3 [==============================] - 0s 43ms/step - loss: 0.1761 - accuracy: 0.9500 - val_loss: 0.2332 - val_accuracy: 1.0000\n","Epoch 9/9\n","3/3 [==============================] - 0s 34ms/step - loss: 0.1654 - accuracy: 0.9789 - val_loss: 0.2069 - val_accuracy: 1.0000\n","Accuracy: 56.52%\n","1/1 [==============================] - 0s 338ms/step\n","[[ 6  9]\n"," [ 5 26]]\n","              precision    recall  f1-score   support\n","\n","           0       0.55      0.40      0.46        15\n","           1       0.74      0.84      0.79        31\n","\n","    accuracy                           0.70        46\n","   macro avg       0.64      0.62      0.62        46\n","weighted avg       0.68      0.70      0.68        46\n","\n","46\n","31\n","67.3913043478261\n","--------------------------------------------------------------------------------------------------------\n","FOOD#PRICES\n","FOOD#PRICES\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 40463.14it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["FOOD#PRICES\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","2/2 [==============================] - 2s 508ms/step - loss: 0.8488 - accuracy: 0.5052 - val_loss: 0.5544 - val_accuracy: 0.7500\n","Epoch 2/9\n","2/2 [==============================] - 0s 32ms/step - loss: 0.4832 - accuracy: 0.7604 - val_loss: 0.4736 - val_accuracy: 0.7500\n","Epoch 3/9\n","2/2 [==============================] - 0s 37ms/step - loss: 0.3139 - accuracy: 0.8654 - val_loss: 0.4475 - val_accuracy: 0.7000\n","Epoch 4/9\n","2/2 [==============================] - 0s 34ms/step - loss: 0.2366 - accuracy: 0.9213 - val_loss: 0.4341 - val_accuracy: 0.8000\n","Epoch 5/9\n","2/2 [==============================] - 0s 38ms/step - loss: 0.1855 - accuracy: 0.9633 - val_loss: 0.4074 - val_accuracy: 0.8000\n","Epoch 6/9\n","2/2 [==============================] - 0s 40ms/step - loss: 0.1606 - accuracy: 0.9860 - val_loss: 0.3826 - val_accuracy: 0.8000\n","Epoch 7/9\n","2/2 [==============================] - 0s 38ms/step - loss: 0.1221 - accuracy: 0.9720 - val_loss: 0.3682 - val_accuracy: 0.8000\n","Epoch 8/9\n","2/2 [==============================] - 0s 36ms/step - loss: 0.1310 - accuracy: 1.0000 - val_loss: 0.3619 - val_accuracy: 0.7500\n","Epoch 9/9\n","2/2 [==============================] - 0s 45ms/step - loss: 0.1012 - accuracy: 1.0000 - val_loss: 0.3587 - val_accuracy: 0.7500\n","Accuracy: 45.00%\n","1/1 [==============================] - 0s 323ms/step\n","[[ 4  2]\n"," [ 4 10]]\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.67      0.57         6\n","           1       0.83      0.71      0.77        14\n","\n","    accuracy                           0.70        20\n","   macro avg       0.67      0.69      0.67        20\n","weighted avg       0.73      0.70      0.71        20\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","2/2 [==============================] - 2s 511ms/step - loss: 0.7053 - accuracy: 0.5088 - val_loss: 0.6443 - val_accuracy: 0.5500\n","Epoch 2/9\n","2/2 [==============================] - 0s 35ms/step - loss: 0.5537 - accuracy: 0.6050 - val_loss: 0.6015 - val_accuracy: 0.6500\n","Epoch 3/9\n","2/2 [==============================] - 0s 37ms/step - loss: 0.4862 - accuracy: 0.7168 - val_loss: 0.4844 - val_accuracy: 0.7000\n","Epoch 4/9\n","2/2 [==============================] - 0s 36ms/step - loss: 0.3202 - accuracy: 0.9441 - val_loss: 0.3890 - val_accuracy: 0.9000\n","Epoch 5/9\n","2/2 [==============================] - 0s 40ms/step - loss: 0.2726 - accuracy: 0.9860 - val_loss: 0.3341 - val_accuracy: 0.9000\n","Epoch 6/9\n","2/2 [==============================] - 0s 35ms/step - loss: 0.2318 - accuracy: 1.0000 - val_loss: 0.3024 - val_accuracy: 0.9000\n","Epoch 7/9\n","2/2 [==============================] - 0s 42ms/step - loss: 0.2354 - accuracy: 1.0000 - val_loss: 0.2805 - val_accuracy: 0.9000\n","Epoch 8/9\n","2/2 [==============================] - 0s 36ms/step - loss: 0.1850 - accuracy: 1.0000 - val_loss: 0.2646 - val_accuracy: 0.9000\n","Epoch 9/9\n","2/2 [==============================] - 0s 39ms/step - loss: 0.1674 - accuracy: 1.0000 - val_loss: 0.2501 - val_accuracy: 0.9500\n","Accuracy: 75.00%\n","1/1 [==============================] - 0s 335ms/step\n","[[5 9]\n"," [1 5]]\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.36      0.50        14\n","           1       0.36      0.83      0.50         6\n","\n","    accuracy                           0.50        20\n","   macro avg       0.60      0.60      0.50        20\n","weighted avg       0.69      0.50      0.50        20\n","\n","20\n","12\n","60.0\n","--------------------------------------------------------------------------------------------------------\n","RESTAURANT#GENERAL\n","RESTAURANT#GENERAL\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 42916.61it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["RESTAURANT#GENERAL\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","8/8 [==============================] - 2s 114ms/step - loss: 0.6484 - accuracy: 0.6225 - val_loss: 0.4968 - val_accuracy: 0.8254\n","Epoch 2/9\n","8/8 [==============================] - 0s 41ms/step - loss: 0.3930 - accuracy: 0.8931 - val_loss: 0.2963 - val_accuracy: 0.8730\n","Epoch 3/9\n","8/8 [==============================] - 0s 43ms/step - loss: 0.3073 - accuracy: 0.9144 - val_loss: 0.3339 - val_accuracy: 0.8730\n","Epoch 4/9\n","8/8 [==============================] - 0s 44ms/step - loss: 0.2674 - accuracy: 0.9382 - val_loss: 0.2116 - val_accuracy: 0.9206\n","Epoch 5/9\n","8/8 [==============================] - 0s 42ms/step - loss: 0.2239 - accuracy: 0.9334 - val_loss: 0.2452 - val_accuracy: 0.8968\n","Epoch 6/9\n","8/8 [==============================] - 0s 43ms/step - loss: 0.1814 - accuracy: 0.9581 - val_loss: 0.1898 - val_accuracy: 0.9206\n","Epoch 7/9\n","8/8 [==============================] - 0s 40ms/step - loss: 0.1692 - accuracy: 0.9580 - val_loss: 0.1573 - val_accuracy: 0.9365\n","Epoch 8/9\n","8/8 [==============================] - 0s 41ms/step - loss: 0.1471 - accuracy: 0.9585 - val_loss: 0.1667 - val_accuracy: 0.9365\n","Epoch 9/9\n","8/8 [==============================] - 0s 42ms/step - loss: 0.1496 - accuracy: 0.9606 - val_loss: 0.1547 - val_accuracy: 0.9365\n","Accuracy: 78.72%\n","3/3 [==============================] - 0s 13ms/step\n","[[71 36]\n"," [14 20]]\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.66      0.74       107\n","           1       0.36      0.59      0.44        34\n","\n","    accuracy                           0.65       141\n","   macro avg       0.60      0.63      0.59       141\n","weighted avg       0.72      0.65      0.67       141\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","8/8 [==============================] - 2s 110ms/step - loss: 0.6629 - accuracy: 0.6435 - val_loss: 0.7540 - val_accuracy: 0.5714\n","Epoch 2/9\n","8/8 [==============================] - 0s 40ms/step - loss: 0.3937 - accuracy: 0.8320 - val_loss: 0.4475 - val_accuracy: 0.7937\n","Epoch 3/9\n","8/8 [==============================] - 0s 41ms/step - loss: 0.3227 - accuracy: 0.8900 - val_loss: 0.4063 - val_accuracy: 0.8413\n","Epoch 4/9\n","8/8 [==============================] - 0s 44ms/step - loss: 0.2469 - accuracy: 0.9258 - val_loss: 0.3076 - val_accuracy: 0.9206\n","Epoch 5/9\n","8/8 [==============================] - 0s 42ms/step - loss: 0.2109 - accuracy: 0.9311 - val_loss: 0.2754 - val_accuracy: 0.9365\n","Epoch 6/9\n","8/8 [==============================] - 0s 43ms/step - loss: 0.1902 - accuracy: 0.9356 - val_loss: 0.2447 - val_accuracy: 0.9524\n","Epoch 7/9\n","8/8 [==============================] - 0s 45ms/step - loss: 0.1705 - accuracy: 0.9486 - val_loss: 0.2327 - val_accuracy: 0.9603\n","Epoch 8/9\n","8/8 [==============================] - 0s 42ms/step - loss: 0.1479 - accuracy: 0.9595 - val_loss: 0.2004 - val_accuracy: 0.9762\n","Epoch 9/9\n","8/8 [==============================] - 0s 44ms/step - loss: 0.1415 - accuracy: 0.9615 - val_loss: 0.1908 - val_accuracy: 0.9762\n","Accuracy: 65.25%\n","3/3 [==============================] - 0s 13ms/step\n","[[  4  30]\n"," [  6 101]]\n","              precision    recall  f1-score   support\n","\n","           0       0.40      0.12      0.18        34\n","           1       0.77      0.94      0.85       107\n","\n","    accuracy                           0.74       141\n","   macro avg       0.59      0.53      0.52       141\n","weighted avg       0.68      0.74      0.69       141\n","\n","141\n","112\n","79.43262411347517\n","--------------------------------------------------------------------------------------------------------\n","RESTAURANT#MISCELLANEOUS\n","RESTAURANT#MISCELLANEOUS\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.7/dist-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_comm.py:596: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  frame = None\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 32212.25it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["RESTAURANT#MISCELLANEOUS\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","2/2 [==============================] - 2s 516ms/step - loss: 0.7034 - accuracy: 0.5550 - val_loss: 0.3507 - val_accuracy: 1.0000\n","Epoch 2/9\n","2/2 [==============================] - 0s 63ms/step - loss: 0.5365 - accuracy: 0.8216 - val_loss: 0.2601 - val_accuracy: 0.9583\n","Epoch 3/9\n","2/2 [==============================] - 0s 56ms/step - loss: 0.3879 - accuracy: 0.9056 - val_loss: 0.1842 - val_accuracy: 0.9583\n","Epoch 4/9\n","2/2 [==============================] - 0s 61ms/step - loss: 0.3015 - accuracy: 0.9502 - val_loss: 0.1197 - val_accuracy: 0.9583\n","Epoch 5/9\n","2/2 [==============================] - 0s 56ms/step - loss: 0.2677 - accuracy: 0.9554 - val_loss: 0.0896 - val_accuracy: 0.9583\n","Epoch 6/9\n","2/2 [==============================] - 0s 63ms/step - loss: 0.2067 - accuracy: 0.9875 - val_loss: 0.0776 - val_accuracy: 0.9583\n","Epoch 7/9\n","2/2 [==============================] - 0s 57ms/step - loss: 0.1955 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9583\n","Epoch 8/9\n","2/2 [==============================] - 0s 71ms/step - loss: 0.1695 - accuracy: 0.9875 - val_loss: 0.0671 - val_accuracy: 0.9583\n","Epoch 9/9\n","2/2 [==============================] - 0s 59ms/step - loss: 0.1661 - accuracy: 0.9875 - val_loss: 0.0573 - val_accuracy: 1.0000\n","Accuracy: 65.52%\n","1/1 [==============================] - 0s 327ms/step\n","[[11  5]\n"," [ 5  8]]\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.69      0.69        16\n","           1       0.62      0.62      0.62        13\n","\n","    accuracy                           0.66        29\n","   macro avg       0.65      0.65      0.65        29\n","weighted avg       0.66      0.66      0.66        29\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","2/2 [==============================] - 2s 521ms/step - loss: 0.7378 - accuracy: 0.4928 - val_loss: 0.9861 - val_accuracy: 0.0833\n","Epoch 2/9\n","2/2 [==============================] - 0s 52ms/step - loss: 0.5222 - accuracy: 0.6721 - val_loss: 0.6520 - val_accuracy: 0.6667\n","Epoch 3/9\n","2/2 [==============================] - 0s 55ms/step - loss: 0.3441 - accuracy: 0.8931 - val_loss: 0.4312 - val_accuracy: 0.8750\n","Epoch 4/9\n","2/2 [==============================] - 0s 65ms/step - loss: 0.2968 - accuracy: 0.8879 - val_loss: 0.3025 - val_accuracy: 0.9583\n","Epoch 5/9\n","2/2 [==============================] - 0s 53ms/step - loss: 0.2365 - accuracy: 0.9554 - val_loss: 0.2436 - val_accuracy: 0.9583\n","Epoch 6/9\n","2/2 [==============================] - 0s 57ms/step - loss: 0.1672 - accuracy: 1.0000 - val_loss: 0.2020 - val_accuracy: 0.9583\n","Epoch 7/9\n","2/2 [==============================] - 0s 74ms/step - loss: 0.1335 - accuracy: 1.0000 - val_loss: 0.1801 - val_accuracy: 0.9583\n","Epoch 8/9\n","2/2 [==============================] - 0s 63ms/step - loss: 0.1260 - accuracy: 0.9751 - val_loss: 0.1650 - val_accuracy: 0.9583\n","Epoch 9/9\n","2/2 [==============================] - 0s 59ms/step - loss: 0.0965 - accuracy: 0.9875 - val_loss: 0.1508 - val_accuracy: 0.9583\n","Accuracy: 51.72%\n","1/1 [==============================] - 0s 322ms/step\n","[[ 0 13]\n"," [ 2 14]]\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00        13\n","           1       0.52      0.88      0.65        16\n","\n","    accuracy                           0.48        29\n","   macro avg       0.26      0.44      0.33        29\n","weighted avg       0.29      0.48      0.36        29\n","\n","29\n","18\n","62.06896551724138\n","--------------------------------------------------------------------------------------------------------\n","RESTAURANT#PRICES\n","RESTAURANT#PRICES\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 41012.66it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["RESTAURANT#PRICES\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.7955 - accuracy: 0.5625 - val_loss: 0.6420 - val_accuracy: 0.5000\n","Epoch 2/9\n","1/1 [==============================] - 0s 68ms/step - loss: 0.6136 - accuracy: 0.6250 - val_loss: 0.5128 - val_accuracy: 0.8125\n","Epoch 3/9\n","1/1 [==============================] - 0s 81ms/step - loss: 0.4153 - accuracy: 0.7812 - val_loss: 0.4267 - val_accuracy: 0.8125\n","Epoch 4/9\n","1/1 [==============================] - 0s 80ms/step - loss: 0.3089 - accuracy: 0.8438 - val_loss: 0.3638 - val_accuracy: 0.8750\n","Epoch 5/9\n","1/1 [==============================] - 0s 89ms/step - loss: 0.3029 - accuracy: 0.8438 - val_loss: 0.3145 - val_accuracy: 0.9375\n","Epoch 6/9\n","1/1 [==============================] - 0s 89ms/step - loss: 0.2477 - accuracy: 0.8906 - val_loss: 0.2747 - val_accuracy: 0.9375\n","Epoch 7/9\n","1/1 [==============================] - 0s 81ms/step - loss: 0.2124 - accuracy: 0.9219 - val_loss: 0.2436 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1727 - accuracy: 0.9844 - val_loss: 0.2186 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1496 - accuracy: 0.9844 - val_loss: 0.1966 - val_accuracy: 1.0000\n","Accuracy: 63.16%\n","1/1 [==============================] - 0s 319ms/step\n","[[ 3  3]\n"," [ 1 12]]\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.50      0.60         6\n","           1       0.80      0.92      0.86        13\n","\n","    accuracy                           0.79        19\n","   macro avg       0.78      0.71      0.73        19\n","weighted avg       0.78      0.79      0.78        19\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.9148 - accuracy: 0.5312 - val_loss: 0.6550 - val_accuracy: 0.3750\n","Epoch 2/9\n","1/1 [==============================] - 0s 68ms/step - loss: 0.5516 - accuracy: 0.5938 - val_loss: 0.6903 - val_accuracy: 0.4375\n","Epoch 3/9\n","1/1 [==============================] - 0s 75ms/step - loss: 0.5337 - accuracy: 0.5469 - val_loss: 0.7249 - val_accuracy: 0.4375\n","Epoch 4/9\n","1/1 [==============================] - 0s 69ms/step - loss: 0.5369 - accuracy: 0.5625 - val_loss: 0.7171 - val_accuracy: 0.4375\n","Epoch 5/9\n","1/1 [==============================] - 0s 86ms/step - loss: 0.4871 - accuracy: 0.5469 - val_loss: 0.6752 - val_accuracy: 0.4375\n","Epoch 6/9\n","1/1 [==============================] - 0s 85ms/step - loss: 0.4251 - accuracy: 0.5469 - val_loss: 0.6171 - val_accuracy: 0.6875\n","Epoch 7/9\n","1/1 [==============================] - 0s 75ms/step - loss: 0.3967 - accuracy: 0.8750 - val_loss: 0.5548 - val_accuracy: 0.6875\n","Epoch 8/9\n","1/1 [==============================] - 0s 90ms/step - loss: 0.3307 - accuracy: 0.9375 - val_loss: 0.4979 - val_accuracy: 0.7500\n","Epoch 9/9\n","1/1 [==============================] - 0s 80ms/step - loss: 0.3048 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 0.7500\n","Accuracy: 63.16%\n","1/1 [==============================] - 0s 324ms/step\n","[[ 0 13]\n"," [ 0  6]]\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00        13\n","           1       0.32      1.00      0.48         6\n","\n","    accuracy                           0.32        19\n","   macro avg       0.16      0.50      0.24        19\n","weighted avg       0.10      0.32      0.15        19\n","\n","19\n","13\n","68.42105263157895\n","--------------------------------------------------------------------------------------------------------\n","SERVICE#GENERAL\n","SERVICE#GENERAL\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 38057.96it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["SERVICE#GENERAL\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","6/6 [==============================] - 2s 163ms/step - loss: 0.7860 - accuracy: 0.4897 - val_loss: 0.4181 - val_accuracy: 0.8242\n","Epoch 2/9\n","6/6 [==============================] - 0s 70ms/step - loss: 0.4225 - accuracy: 0.8085 - val_loss: 0.3563 - val_accuracy: 0.8571\n","Epoch 3/9\n","6/6 [==============================] - 0s 69ms/step - loss: 0.3148 - accuracy: 0.8923 - val_loss: 0.3448 - val_accuracy: 0.8571\n","Epoch 4/9\n","6/6 [==============================] - 0s 64ms/step - loss: 0.2661 - accuracy: 0.9171 - val_loss: 0.2949 - val_accuracy: 0.8791\n","Epoch 5/9\n","6/6 [==============================] - 0s 67ms/step - loss: 0.1967 - accuracy: 0.9475 - val_loss: 0.2866 - val_accuracy: 0.8791\n","Epoch 6/9\n","6/6 [==============================] - 0s 67ms/step - loss: 0.1731 - accuracy: 0.9555 - val_loss: 0.3380 - val_accuracy: 0.8242\n","Epoch 7/9\n","6/6 [==============================] - 0s 67ms/step - loss: 0.1614 - accuracy: 0.9580 - val_loss: 0.2581 - val_accuracy: 0.8901\n","Epoch 8/9\n","6/6 [==============================] - 0s 68ms/step - loss: 0.1322 - accuracy: 0.9697 - val_loss: 0.2519 - val_accuracy: 0.8791\n","Epoch 9/9\n","6/6 [==============================] - 0s 71ms/step - loss: 0.1101 - accuracy: 0.9871 - val_loss: 0.2907 - val_accuracy: 0.8571\n","Accuracy: 72.30%\n","3/3 [==============================] - 0s 17ms/step\n","[[48 24]\n"," [10 66]]\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.67      0.74        72\n","           1       0.73      0.87      0.80        76\n","\n","    accuracy                           0.77       148\n","   macro avg       0.78      0.77      0.77       148\n","weighted avg       0.78      0.77      0.77       148\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","6/6 [==============================] - 2s 152ms/step - loss: 0.7306 - accuracy: 0.5443 - val_loss: 0.5936 - val_accuracy: 0.6813\n","Epoch 2/9\n","6/6 [==============================] - 0s 66ms/step - loss: 0.4823 - accuracy: 0.7702 - val_loss: 0.4941 - val_accuracy: 0.7692\n","Epoch 3/9\n","6/6 [==============================] - 0s 67ms/step - loss: 0.3755 - accuracy: 0.8633 - val_loss: 0.5060 - val_accuracy: 0.7582\n","Epoch 4/9\n","6/6 [==============================] - 0s 65ms/step - loss: 0.2855 - accuracy: 0.9132 - val_loss: 0.3909 - val_accuracy: 0.8462\n","Epoch 5/9\n","6/6 [==============================] - 0s 69ms/step - loss: 0.2601 - accuracy: 0.9202 - val_loss: 0.3894 - val_accuracy: 0.8571\n","Epoch 6/9\n","6/6 [==============================] - 0s 60ms/step - loss: 0.2427 - accuracy: 0.9255 - val_loss: 0.3823 - val_accuracy: 0.8681\n","Epoch 7/9\n","6/6 [==============================] - 0s 59ms/step - loss: 0.2053 - accuracy: 0.9427 - val_loss: 0.3663 - val_accuracy: 0.8681\n","Epoch 8/9\n","6/6 [==============================] - 0s 67ms/step - loss: 0.1699 - accuracy: 0.9598 - val_loss: 0.3735 - val_accuracy: 0.8791\n","Epoch 9/9\n","6/6 [==============================] - 0s 70ms/step - loss: 0.1801 - accuracy: 0.9423 - val_loss: 0.3378 - val_accuracy: 0.8791\n","Accuracy: 75.68%\n","3/3 [==============================] - 0s 19ms/step\n","[[40 36]\n"," [11 61]]\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.53      0.63        76\n","           1       0.63      0.85      0.72        72\n","\n","    accuracy                           0.68       148\n","   macro avg       0.71      0.69      0.68       148\n","weighted avg       0.71      0.68      0.67       148\n","\n","148\n","114\n","77.02702702702703\n","--------------------------------------------------------------------------------------------------------\n","LOCATION#GENERAL\n","LOCATION#GENERAL\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/root/nltk_data/corpora/stopwords/english'>\n","  return concat([self.open(f).read() for f in fileids])\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","\n","\n","Grid search: 100%|██████████| 216/216 [00:00<00:00, 52223.29it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["LOCATION#GENERAL\n","negative\n","Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.7703 - accuracy: 0.7273 - val_loss: 0.2122 - val_accuracy: 1.0000\n","Epoch 2/9\n","1/1 [==============================] - 0s 54ms/step - loss: 0.5262 - accuracy: 0.3636 - val_loss: 0.0583 - val_accuracy: 1.0000\n","Epoch 3/9\n","1/1 [==============================] - 0s 51ms/step - loss: 0.4920 - accuracy: 0.3636 - val_loss: 0.0205 - val_accuracy: 1.0000\n","Epoch 4/9\n","1/1 [==============================] - 0s 49ms/step - loss: 0.4855 - accuracy: 0.3636 - val_loss: 0.0093 - val_accuracy: 1.0000\n","Epoch 5/9\n","1/1 [==============================] - 0s 48ms/step - loss: 0.4950 - accuracy: 0.9697 - val_loss: 0.0052 - val_accuracy: 1.0000\n","Epoch 6/9\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4995 - accuracy: 0.9697 - val_loss: 0.0034 - val_accuracy: 1.0000\n","Epoch 7/9\n","1/1 [==============================] - 0s 43ms/step - loss: 0.4898 - accuracy: 0.9697 - val_loss: 0.0024 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 42ms/step - loss: 0.4850 - accuracy: 0.9697 - val_loss: 0.0019 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 64ms/step - loss: 0.4728 - accuracy: 0.9697 - val_loss: 0.0016 - val_accuracy: 1.0000\n","Accuracy: 100.00%\n","1/1 [==============================] - 0s 310ms/step\n","[[ 0 11]\n"," [ 0  0]]\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      11.0\n","           1       0.00      0.00      0.00       0.0\n","\n","    accuracy                           0.00      11.0\n","   macro avg       0.00      0.00      0.00      11.0\n","weighted avg       0.00      0.00      0.00      11.0\n","\n","positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 1193514 word vectors.\n","Epoch 1/9\n","1/1 [==============================] - 2s 2s/step - loss: 0.9050 - accuracy: 0.2727 - val_loss: 0.5363 - val_accuracy: 1.0000\n","Epoch 2/9\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4607 - accuracy: 0.8182 - val_loss: 0.3662 - val_accuracy: 1.0000\n","Epoch 3/9\n","1/1 [==============================] - 0s 49ms/step - loss: 0.2099 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 1.0000\n","Epoch 4/9\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1035 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 1.0000\n","Epoch 5/9\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 1.0000\n","Epoch 6/9\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 1.0000\n","Epoch 7/9\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 1.0000\n","Epoch 8/9\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n","Epoch 9/9\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n","Accuracy: 100.00%\n","1/1 [==============================] - 0s 321ms/step\n","[[11]]\n","              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        11\n","   macro avg       1.00      1.00      1.00        11\n","weighted avg       1.00      1.00      1.00        11\n","\n","11\n","11\n","100.0\n","--------------------------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"utlYLotJh48x","executionInfo":{"status":"ok","timestamp":1622437957881,"user_tz":300,"elapsed":343,"user":{"displayName":"Monserrat Vázquez Hernández","photoUrl":"","userId":"12458819110641506711"}},"outputId":"247f069c-dab4-4594-c71a-7466550d1578"},"source":["  categorias_list = [\"AMBIENCE#GENERAL\", \"DRINKS#QUALITY\",\"DRINKS#STYLE_OPTIONS\",\"DRINKS#PRICES\",\"FOOD#QUALITY\", \"FOOD#STYLE_OPTIONS\",\n","             \"FOOD#PRICES\",\"RESTAURANT#GENERAL\", \"RESTAURANT#MISCELLANEOUS\",\"RESTAURANT#PRICES\",\"SERVICE#GENERAL\",\"LOCATION#GENERAL\"]\n","\n","  #------------------------------------------------------------------------------------------------------------\n","  #Cargar datos de entrenamiento\n","  d_train = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TRAIN.reviews.gold','r', encoding='utf-8')\n","  d_train_ = []\n","  for line in d_train.readlines():\n","    d_train_.append(line.replace(\"\\n\",\"\"))\n","  d_train.close()\n","\n","  #Cargar clases de entrenamiento\n","  l_train = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TRAIN.class.gold','r', encoding='utf-8')\n","\n","  l_train_ = []\n","  for line in l_train.readlines():\n","    l_train_.append(line.replace(\"\\n\",\"\"))\n","\n","  l_train.close()\n","\n","  #Cargar polaridades de entrenamiento\n","  p_train = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TRAIN.polarities.gold','r', encoding='utf-8')\n","\n","  p_train_ = []\n","  for line in p_train.readlines():\n","    p_train_.append(line.replace(\"\\n\",\"\"))\n","\n","  p_train.close()\n","\n","  # print(p_train_)\n","\n","\n","\n","  for category in categorias_list:\n","    x_train = []\n","    y_train = []\n","\n","    for index_review in range(len(d_train_)):\n","      if l_train_[index_review] == category:\n","        x_train.append(d_train_[index_review])\n","        y_train.append(p_train_[index_review])\n","\n","    neutral = 0\n","    positive = 0\n","    negative = 0\n","    for index_review in range(len(x_train)):\n","      if y_train[index_review] == 'neutral':\n","        neutral += 1\n","      else:\n","        if y_train[index_review] == 'positive':\n","          positive +=1\n","        else:\n","          if y_train[index_review] == 'negative':\n","            negative += 1\n","\n","    print(category)\n","    print('NEU:' + str(neutral))\n","    print('POS:' + str(positive))\n","    print('NEG:' + str(negative))\n","    print(neutral + positive + negative)\n","    print(\"---------------------------\")\n","\n","\n","    # x_train= CleanDocs(x_train)\n","\n","  #------------------------------------------------------------------------------------------------------------\n","\n","  print(\"#------------------------------------------------------------------------------------------------------------\")\n","  #------------------------------------------------------------------------------------------------------------\n","  #Cargar datos de entrenamiento\n","  d_test = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TEST.reviews.gold','r', encoding='utf-8')\n","  d_test_ = []\n","  for line in d_test.readlines():\n","    d_test_.append(line.replace(\"\\n\",\"\"))\n","  d_test.close()\n","\n","  #Cargar clases de entrenamiento\n","  l_test = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TEST.class.gold','r', encoding='utf-8')\n","\n","  l_test_ = []\n","  for line in l_test.readlines():\n","    l_test_.append(line.replace(\"\\n\",\"\"))\n","  l_test.close()\n","\n","  #Cargar polaridades de entrenamiento\n","  p_test = open('/content/drive/MyDrive/ProyectoTesis_INAOE/restaurante_EN/SLOT3/S3-V3_EN_REST_SB1_TEST.polarities.gold','r', encoding='utf-8')\n","\n","  p_test_ = []\n","  for line in p_test.readlines():\n","    p_test_.append(line.replace(\"\\n\",\"\"))\n","\n","  p_test.close()\n","\n","\n","\n","  for category in categorias_list:\n","    x_test = []\n","    y_test = []\n","\n","    for index_review in range(len(d_test_)):\n","      if l_test_[index_review] == category:\n","        x_test.append(d_test_[index_review])\n","        y_test.append(p_test_[index_review])\n","\n","    neutral = 0\n","    positive = 0\n","    negative = 0\n","    for index_review in range(len(x_test)):\n","      if y_test[index_review] == 'neutral':\n","        neutral += 1\n","      else:\n","        if y_test[index_review] == 'positive':\n","          positive +=1\n","        else:\n","          if y_test[index_review] == 'negative':\n","            negative += 1\n","\n","    print(category)\n","    print('NEU:' + str(neutral))\n","    print('POS:' + str(positive))\n","    print('NEG:' + str(negative))\n","    print(neutral + positive + negative)\n","    print(\"---------------------------\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AMBIENCE#GENERAL\n","NEU:16\n","POS:197\n","NEG:42\n","255\n","---------------------------\n","DRINKS#QUALITY\n","NEU:2\n","POS:40\n","NEG:5\n","47\n","---------------------------\n","DRINKS#STYLE_OPTIONS\n","NEU:0\n","POS:29\n","NEG:3\n","32\n","---------------------------\n","DRINKS#PRICES\n","NEU:0\n","POS:13\n","NEG:7\n","20\n","---------------------------\n","FOOD#QUALITY\n","NEU:28\n","POS:617\n","NEG:204\n","849\n","---------------------------\n","FOOD#STYLE_OPTIONS\n","NEU:9\n","POS:83\n","NEG:45\n","137\n","---------------------------\n","FOOD#PRICES\n","NEU:1\n","POS:41\n","NEG:48\n","90\n","---------------------------\n","RESTAURANT#GENERAL\n","NEU:8\n","POS:313\n","NEG:101\n","422\n","---------------------------\n","RESTAURANT#MISCELLANEOUS\n","NEU:13\n","POS:58\n","NEG:27\n","98\n","---------------------------\n","RESTAURANT#PRICES\n","NEU:6\n","POS:34\n","NEG:40\n","80\n","---------------------------\n","SERVICE#GENERAL\n","NEU:12\n","POS:211\n","NEG:226\n","449\n","---------------------------\n","LOCATION#GENERAL\n","NEU:6\n","POS:21\n","NEG:1\n","28\n","---------------------------\n","#------------------------------------------------------------------------------------------------------------\n","AMBIENCE#GENERAL\n","NEU:3\n","POS:61\n","NEG:2\n","66\n","---------------------------\n","DRINKS#QUALITY\n","NEU:0\n","POS:21\n","NEG:1\n","22\n","---------------------------\n","DRINKS#STYLE_OPTIONS\n","NEU:0\n","POS:11\n","NEG:1\n","12\n","---------------------------\n","DRINKS#PRICES\n","NEU:0\n","POS:0\n","NEG:4\n","4\n","---------------------------\n","FOOD#QUALITY\n","NEU:13\n","POS:269\n","NEG:31\n","313\n","---------------------------\n","FOOD#STYLE_OPTIONS\n","NEU:9\n","POS:31\n","NEG:15\n","55\n","---------------------------\n","FOOD#PRICES\n","NEU:3\n","POS:6\n","NEG:14\n","23\n","---------------------------\n","RESTAURANT#GENERAL\n","NEU:1\n","POS:107\n","NEG:34\n","142\n","---------------------------\n","RESTAURANT#MISCELLANEOUS\n","NEU:4\n","POS:16\n","NEG:13\n","33\n","---------------------------\n","RESTAURANT#PRICES\n","NEU:2\n","POS:6\n","NEG:13\n","21\n","---------------------------\n","SERVICE#GENERAL\n","NEU:7\n","POS:72\n","NEG:76\n","155\n","---------------------------\n","LOCATION#GENERAL\n","NEU:2\n","POS:11\n","NEG:0\n","13\n","---------------------------\n"],"name":"stdout"}]}]}